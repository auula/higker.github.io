<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Redis那些事随笔</title>
    <link href="/2020/08/04/Redis%E9%82%A3%E4%BA%9B%E4%BA%8B%E9%9A%8F%E7%AC%94/"/>
    <url>/2020/08/04/Redis%E9%82%A3%E4%BA%9B%E4%BA%8B%E9%9A%8F%E7%AC%94/</url>
    
    <content type="html"><![CDATA[<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghew9a67sqj30jg07bt8y.jpg" srcset="/img/loading.gif" alt=""></p><p><strong><code>Redis</code>（<code>Remote Dictionary Server</code>)，即<code>远程字典服务</code>，是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、<code>Key-Value数据库</code>，并提供多种语言的API。从2010年3月15日起，Redis的开发工作由VMware主持。从2013年5月开始，Redis的开发由Pivotal赞助。它通常被称为数据结构服务器，因为值（value）可以是 字符串(String), 哈希(Hash), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型。</strong></p><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><ul><li>String: 字符串</li><li>Hash: 散列</li><li>List: 列表</li><li>Set: 集合</li><li>Sorted Set: 有序集合</li><li>还有新加的里地理位置等类型</li></ul><h3 id="特点和缺点"><a href="#特点和缺点" class="headerlink" title="特点和缺点"></a>特点和缺点</h3><ul><li>内存存储，读写操作快</li><li>效率高，可用于高速缓存</li><li>发布订阅系统</li><li>地图信息分析</li><li>计时器，计数器（浏览量!）</li><li>流量限流，消息队列消峰</li><li><code>断电内存数据丢失（所有持久化很重要）</code></li><li><code>RDB，AOF 持久化策略</code></li></ul><h3 id="安装Redis"><a href="#安装Redis" class="headerlink" title="安装Redis"></a>安装Redis</h3><p><strong>1. Linux解决安装一些依赖问题</strong></p><pre><code class="bash">yum groups install Development Tools</code></pre><p><strong>2. 下载解压编译安装</strong></p><pre><code class="bash">$ wget http://download.redis.io/releases/redis-5.0.5.tar.gz$ tar xzf redis-5.0.5.tar.gz$ cd redis-5.0.5$ make</code></pre><p>进入到解压后的 src 目录，通过如下命令启动Redis:</p><p><code>$ src/redis-server</code></p><p>您可以使用内置的客户端与Redis进行交互:</p><pre><code class="bash">$ src/redis-cliredis&gt; set foo barOKredis&gt; get foo&quot;bar&quot;</code></pre><p>ok大功告成。</p><h3 id="配置Redis-conf文件"><a href="#配置Redis-conf文件" class="headerlink" title="配置Redis.conf文件"></a>配置<code>Redis.conf</code>文件</h3><pre><code class="json"># Redis configuration file example.requirepass passwordmaxclients 10000## Note that in order to read the configuration file, Redis must be# started with the file path as first argument:## ./redis-server /path/to/redis.conf# Note on units: when memory size is needed, it is possible to specify# it in the usual form of 1k 5GB 4M and so forth:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.################################## INCLUDES #################################### Include one or more other config files here.  This is useful if you# have a standard template that goes to all Redis servers but also need# to customize a few per-server settings.  Include files can include# other files, so use this wisely.## Notice option &quot;include&quot; won&#39;t be rewritten by command &quot;CONFIG REWRITE&quot;# from admin or Redis Sentinel. Since Redis always uses the last processed# line as value of a configuration directive, you&#39;d better put includes# at the beginning of this file to avoid overwriting config change at runtime.## If instead you are interested in using includes to override configuration# options, it is better to use include as the last line.## include /path/to/local.conf# include /path/to/other.conf################################## MODULES ###################################### Load modules at startup. If the server is not able to load modules# it will abort. It is possible to use multiple loadmodule directives.## loadmodule /path/to/my_module.so# loadmodule /path/to/other_module.so################################## NETWORK ###################################### By default, if no &quot;bind&quot; configuration directive is specified, Redis listens# for connections from all the network interfaces available on the server.# It is possible to listen to just one or multiple selected interfaces using# the &quot;bind&quot; configuration directive, followed by one or more IP addresses.## Examples:## bind 192.168.1.100 10.0.0.1# bind 127.0.0.1 ::1## ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the# internet, binding to all the interfaces is dangerous and will expose the# instance to everybody on the internet. So by default we uncomment the# following bind directive, that will force Redis to listen only into# the IPv4 loopback interface address (this means Redis will be able to# accept connections only from clients running into the same computer it# is running).## IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES# JUST COMMENT THE FOLLOWING LINE.# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~# bind 128.199.155.162# Protected mode is a layer of security protection, in order to avoid that# Redis instances left open on the internet are accessed and exploited.## When protected mode is on and if:## 1) The server is not binding explicitly to a set of addresses using the#    &quot;bind&quot; directive.# 2) No password is configured.## The server only accepts connections from clients connecting from the# IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain# sockets.## By default protected mode is enabled. You should disable it only if# you are sure you want clients from other hosts to connect to Redis# even if no authentication is configured, nor a specific set of interfaces# are explicitly listed using the &quot;bind&quot; directive.protected-mode no# Accept connections on the specified port, default is 6379 (IANA #815344).# If port 0 is specified Redis will not listen on a TCP socket.port 6379# TCP listen() backlog.## In high requests-per-second environments you need an high backlog in order# to avoid slow clients connections issues. Note that the Linux kernel# will silently truncate it to the value of /proc/sys/net/core/somaxconn so# make sure to raise both the value of somaxconn and tcp_max_syn_backlog# in order to get the desired effect.tcp-backlog 511# Unix socket.## Specify the path for the Unix socket that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /tmp/redis.sock# unixsocketperm 700# Close the connection after a client is idle for N seconds (0 to disable)timeout 0# TCP keepalive.## If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence# of communication. This is useful for two reasons:## 1) Detect dead peers.# 2) Take the connection alive from the point of view of network#    equipment in the middle.## On Linux, the specified value (in seconds) is the period used to send ACKs.# Note that to close the connection the double of the time is needed.# On other kernels the period depends on the kernel configuration.## A reasonable value for this option is 300 seconds, which is the new# Redis default starting with Redis 3.2.1.tcp-keepalive 300################################# GENERAL ###################################### By default Redis does not run as a daemon. Use &#39;yes&#39; if you need it.# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.daemonize yes # If you run Redis from upstart or systemd, Redis can interact with your# supervision tree. Options:#   supervised no      - no supervision interaction#   supervised upstart - signal upstart by putting Redis into SIGSTOP mode#   supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET#   supervised auto    - detect upstart or systemd method based on#                        UPSTART_JOB or NOTIFY_SOCKET environment variables# Note: these supervision methods only signal &quot;process is ready.&quot;#       They do not enable continuous liveness pings back to your supervisor.supervised no# If a pid file is specified, Redis writes it where specified at startup# and removes it at exit.## When the server runs non daemonized, no pid file is created if none is# specified in the configuration. When the server is daemonized, the pid file# is used even if not specified, defaulting to &quot;/var/run/redis.pid&quot;.## Creating a pid file is best effort: if Redis is not able to create it# nothing bad happens, the server will start and run normally.pidfile /www/server/redis/redis.pid # Specify the server verbosity level.# This can be one of:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel notice# Specify the log file name. Also the empty string can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/nulllogfile &quot;/www/server/redis/redis.log&quot;# To enable logging to the system logger, just set &#39;syslog-enabled&#39; to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and &#39;databases&#39;-1databases 16# By default Redis shows an ASCII art logo only when started to log to the# standard output and if the standard output is a TTY. Basically this means# that normally a logo is displayed only in interactive sessions.## However it is possible to force the pre-4.0 behavior and always show a# ASCII art logo in startup logs by setting the following option to yes.always-show-logo yes################################ SNAPSHOTTING  ################################## Save the DB on disk:##   save &lt;seconds&gt; &lt;changes&gt;##   Will save the DB if both the given number of seconds and the given#   number of write operations against the DB occurred.##   In the example below the behaviour will be to save:#   after 900 sec (15 min) if at least 1 key changed#   after 300 sec (5 min) if at least 10 keys changed#   after 60 sec if at least 10000 keys changed##   Note: you can disable saving completely by commenting out all &quot;save&quot; lines.##   It is also possible to remove all the previously configured save#   points by adding a save directive with a single empty string argument#   like in the following example:##   save &quot;&quot;save 900 1save 300 10save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases?# For default that&#39;s set to &#39;yes&#39; as it&#39;s almost always a win.# If you want to save some CPU in the saving child set it to &#39;no&#39; but# the dataset will likely be bigger if you have compressible values or keys.rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check.rdbchecksum yes# The filename where to dump the DBdbfilename dump.rdb# The working directory.## The DB will be written inside this directory, with the filename specified# above using the &#39;dbfilename&#39; configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.dir /www/server/redis/################################# REPLICATION ################################## Master-Replica replication. Use replicaof to make a Redis instance a copy of# another Redis server. A few things to understand ASAP about Redis replication.##   +------------------+      +---------------+#   |      Master      | ---&gt; |    Replica    |#   | (receive writes) |      |  (exact copy) |#   +------------------+      +---------------+## 1) Redis replication is asynchronous, but you can configure a master to#    stop accepting writes if it appears to be not connected with at least#    a given number of replicas.# 2) Redis replicas are able to perform a partial resynchronization with the#    master if the replication link is lost for a relatively small amount of#    time. You may want to configure the replication backlog size (see the next#    sections of this file) with a sensible value depending on your needs.# 3) Replication is automatic and does not need user intervention. After a#    network partition replicas automatically try to reconnect to masters#    and resynchronize with them.## replicaof &lt;masterip&gt; &lt;masterport&gt;# If the master is password protected (using the &quot;requirepass&quot; configuration# directive below) it is possible to tell the replica to authenticate before# starting the replication synchronization process, otherwise the master will# refuse the replica request.## masterauth &lt;master-password&gt;# When a replica loses its connection with the master, or when the replication# is still in progress, the replica can act in two different ways:## 1) if replica-serve-stale-data is set to &#39;yes&#39; (the default) the replica will#    still reply to client requests, possibly with out of date data, or the#    data set may just be empty if this is the first synchronization.## 2) if replica-serve-stale-data is set to &#39;no&#39; the replica will reply with#    an error &quot;SYNC with master in progress&quot; to all the kind of commands#    but to INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG,#    SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB,#    COMMAND, POST, HOST: and LATENCY.#replica-serve-stale-data yes# You can configure a replica instance to accept writes or not. Writing against# a replica instance may be useful to store some ephemeral data (because data# written on a replica will be easily deleted after resync with the master) but# may also cause problems if clients are writing to it because of a# misconfiguration.## Since Redis 2.6 by default replicas are read-only.## Note: read only replicas are not designed to be exposed to untrusted clients# on the internet. It&#39;s just a protection layer against misuse of the instance.# Still a read only replica exports by default all the administrative commands# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve# security of read only replicas using &#39;rename-command&#39; to shadow all the# administrative / dangerous commands.replica-read-only yes# Replication SYNC strategy: disk or socket.## -------------------------------------------------------# WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY# -------------------------------------------------------## New replicas and reconnecting replicas that are not able to continue the replication# process just receiving differences, need to do what is called a &quot;full# synchronization&quot;. An RDB file is transmitted from the master to the replicas.# The transmission can happen in two different ways:## 1) Disk-backed: The Redis master creates a new process that writes the RDB#                 file on disk. Later the file is transferred by the parent#                 process to the replicas incrementally.# 2) Diskless: The Redis master creates a new process that directly writes the#              RDB file to replica sockets, without touching the disk at all.## With disk-backed replication, while the RDB file is generated, more replicas# can be queued and served with the RDB file as soon as the current child producing# the RDB file finishes its work. With diskless replication instead once# the transfer starts, new replicas arriving will be queued and a new transfer# will start when the current one terminates.## When diskless replication is used, the master waits a configurable amount of# time (in seconds) before starting the transfer in the hope that multiple replicas# will arrive and the transfer can be parallelized.## With slow disks and fast (large bandwidth) networks, diskless replication# works better.repl-diskless-sync no# When diskless replication is enabled, it is possible to configure the delay# the server waits in order to spawn the child that transfers the RDB via socket# to the replicas.## This is important since once the transfer starts, it is not possible to serve# new replicas arriving, that will be queued for the next RDB transfer, so the server# waits a delay in order to let more replicas arrive.## The delay is specified in seconds, and by default is 5 seconds. To disable# it entirely just set it to 0 seconds and the transfer will start ASAP.repl-diskless-sync-delay 5# Replicas send PINGs to server in a predefined interval. It&#39;s possible to change# this interval with the repl_ping_replica_period option. The default value is 10# seconds.## repl-ping-replica-period 10# The following option sets the replication timeout for:## 1) Bulk transfer I/O during SYNC, from the point of view of replica.# 2) Master timeout from the point of view of replicas (data, pings).# 3) Replica timeout from the point of view of masters (REPLCONF ACK pings).## It is important to make sure that this value is greater than the value# specified for repl-ping-replica-period otherwise a timeout will be detected# every time there is low traffic between the master and the replica.## repl-timeout 60# Disable TCP_NODELAY on the replica socket after SYNC?## If you select &quot;yes&quot; Redis will use a smaller number of TCP packets and# less bandwidth to send data to replicas. But this can add a delay for# the data to appear on the replica side, up to 40 milliseconds with# Linux kernels using a default configuration.## If you select &quot;no&quot; the delay for data to appear on the replica side will# be reduced but more bandwidth will be used for replication.## By default we optimize for low latency, but in very high traffic conditions# or when the master and replicas are many hops away, turning this to &quot;yes&quot; may# be a good idea.repl-disable-tcp-nodelay no# Set the replication backlog size. The backlog is a buffer that accumulates# replica data when replicas are disconnected for some time, so that when a replica# wants to reconnect again, often a full resync is not needed, but a partial# resync is enough, just passing the portion of data the replica missed while# disconnected.## The bigger the replication backlog, the longer the time the replica can be# disconnected and later be able to perform a partial resynchronization.## The backlog is only allocated once there is at least a replica connected.## repl-backlog-size 1mb# After a master has no longer connected replicas for some time, the backlog# will be freed. The following option configures the amount of seconds that# need to elapse, starting from the time the last replica disconnected, for# the backlog buffer to be freed.## Note that replicas never free the backlog for timeout, since they may be# promoted to masters later, and should be able to correctly &quot;partially# resynchronize&quot; with the replicas: hence they should always accumulate backlog.## A value of 0 means to never release the backlog.## repl-backlog-ttl 3600# The replica priority is an integer number published by Redis in the INFO output.# It is used by Redis Sentinel in order to select a replica to promote into a# master if the master is no longer working correctly.## A replica with a low priority number is considered better for promotion, so# for instance if there are three replicas with priority 10, 100, 25 Sentinel will# pick the one with priority 10, that is the lowest.## However a special priority of 0 marks the replica as not able to perform the# role of master, so a replica with priority of 0 will never be selected by# Redis Sentinel for promotion.## By default the priority is 100.replica-priority 100# It is possible for a master to stop accepting writes if there are less than# N replicas connected, having a lag less or equal than M seconds.## The N replicas need to be in &quot;online&quot; state.## The lag in seconds, that must be &lt;= the specified value, is calculated from# the last ping received from the replica, that is usually sent every second.## This option does not GUARANTEE that N replicas will accept the write, but# will limit the window of exposure for lost writes in case not enough replicas# are available, to the specified number of seconds.## For example to require at least 3 replicas with a lag &lt;= 10 seconds use:## min-replicas-to-write 3# min-replicas-max-lag 10## Setting one or the other to 0 disables the feature.## By default min-replicas-to-write is set to 0 (feature disabled) and# min-replicas-max-lag is set to 10.# A Redis master is able to list the address and port of the attached# replicas in different ways. For example the &quot;INFO replication&quot; section# offers this information, which is used, among other tools, by# Redis Sentinel in order to discover replica instances.# Another place where this info is available is in the output of the# &quot;ROLE&quot; command of a master.## The listed IP and address normally reported by a replica is obtained# in the following way:##   IP: The address is auto detected by checking the peer address#   of the socket used by the replica to connect with the master.##   Port: The port is communicated by the replica during the replication#   handshake, and is normally the port that the replica is using to#   listen for connections.## However when port forwarding or Network Address Translation (NAT) is# used, the replica may be actually reachable via different IP and port# pairs. The following two options can be used by a replica in order to# report to its master a specific set of IP and port, so that both INFO# and ROLE will report those values.## There is no need to use both the options if you need to override just# the port or the IP address.## replica-announce-ip 5.5.5.5# replica-announce-port 1234################################## SECURITY #################################### Require clients to issue AUTH &lt;PASSWORD&gt; before processing any other# commands.  This might be useful in environments in which you do not trust# others with access to the host running redis-server.## This should stay commented out for backward compatibility and because most# people do not need auth (e.g. they run their own servers).## Warning: since Redis is pretty fast an outside user can try up to# 150k passwords per second against a good box. This means that you should# use a very strong password otherwise it will be very easy to break.## requirepass foobared# Command renaming.## It is possible to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# hard to guess so that it will still be available for internal-use tools# but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possible to completely kill a command by renaming it into# an empty string:## rename-command CONFIG &quot;&quot;## Please note that changing the name of commands that are logged into the# AOF file or transmitted to replicas may cause problems.################################### CLIENTS ##################################### Set the max number of connected clients at the same time. By default# this limit is set to 10000 clients, however if the Redis server is not# able to configure the process file limit to allow for the specified limit# the max number of allowed clients is set to the current file limit# minus 32 (as Redis reserves a few file descriptors for internal uses).## Once the limit is reached Redis will close all the new connections sending# an error &#39;max number of clients reached&#39;.## maxclients 10000############################## MEMORY MANAGEMENT ################################# Set a memory usage limit to the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys# according to the eviction policy selected (see maxmemory-policy).## If Redis can&#39;t remove keys according to the policy, or if the policy is# set to &#39;noeviction&#39;, Redis will start to reply with errors to commands# that would use more memory, like SET, LPUSH, and so on, and will continue# to reply to read-only commands like GET.## This option is usually useful when using Redis as an LRU or LFU cache, or to# set a hard memory limit for an instance (using the &#39;noeviction&#39; policy).## WARNING: If you have replicas attached to an instance with maxmemory on,# the size of the output buffers needed to feed the replicas are subtracted# from the used memory count, so that network problems / resyncs will# not trigger a loop where keys are evicted, and in turn the output# buffer of replicas is full with DELs of keys evicted triggering the deletion# of more keys, and so forth until the database is completely emptied.## In short... if you have replicas attached it is suggested that you set a lower# limit for maxmemory so that there is some free RAM on the system for replica# output buffers (but this is not needed if the policy is &#39;noeviction&#39;).## maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached. You can select among five behaviors:## volatile-lru -&gt; Evict using approximated LRU among the keys with an expire set.# allkeys-lru -&gt; Evict any key using approximated LRU.# volatile-lfu -&gt; Evict using approximated LFU among the keys with an expire set.# allkeys-lfu -&gt; Evict any key using approximated LFU.# volatile-random -&gt; Remove a random key among the ones with an expire set.# allkeys-random -&gt; Remove a random key, any key.# volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)# noeviction -&gt; Don&#39;t evict anything, just return an error on write operations.## LRU means Least Recently Used# LFU means Least Frequently Used## Both LRU, LFU and volatile-ttl are implemented using approximated# randomized algorithms.## Note: with any of the above policies, Redis will return an error on write#       operations, when there are no suitable keys for eviction.##       At the date of writing these commands are: set setnx setex append#       incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd#       sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby#       zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby#       getset mset msetnx exec sort## The default is:## maxmemory-policy noeviction# LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated# algorithms (in order to save memory), so you can tune it for speed or# accuracy. For default Redis will check five keys and pick the one that was# used less recently, you can change the sample size using the following# configuration directive.## The default of 5 produces good enough results. 10 Approximates very closely# true LRU but costs more CPU. 3 is faster but not very accurate.## maxmemory-samples 5# Starting from Redis 5, by default a replica will ignore its maxmemory setting# (unless it is promoted to master after a failover or manually). It means# that the eviction of keys will be just handled by the master, sending the# DEL commands to the replica as keys evict in the master side.## This behavior ensures that masters and replicas stay consistent, and is usually# what you want, however if your replica is writable, or you want the replica to have# a different memory setting, and you are sure all the writes performed to the# replica are idempotent, then you may change this default (but be sure to understand# what you are doing).## Note that since the replica by default does not evict, it may end using more# memory than the one set via maxmemory (there are certain buffers that may# be larger on the replica, or data structures may sometimes take more memory and so# forth). So make sure you monitor your replicas and make sure they have enough# memory to never hit a real out-of-memory condition before the master hits# the configured maxmemory setting.## replica-ignore-maxmemory yes############################# LAZY FREEING ##################################### Redis has two primitives to delete keys. One is called DEL and is a blocking# deletion of the object. It means that the server stops processing new commands# in order to reclaim all the memory associated with an object in a synchronous# way. If the key deleted is associated with a small object, the time needed# in order to execute the DEL command is very small and comparable to most other# O(1) or O(log_N) commands in Redis. However if the key is associated with an# aggregated value containing millions of elements, the server can block for# a long time (even seconds) in order to complete the operation.## For the above reasons Redis also offers non blocking deletion primitives# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and# FLUSHDB commands, in order to reclaim memory in background. Those commands# are executed in constant time. Another thread will incrementally free the# object in the background as fast as possible.## DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.# It&#39;s up to the design of the application to understand when it is a good# idea to use one or the other. However the Redis server sometimes has to# delete keys or flush the whole database as a side effect of other operations.# Specifically Redis deletes objects independently of a user call in the# following scenarios:## 1) On eviction, because of the maxmemory and maxmemory policy configurations,#    in order to make room for new data, without going over the specified#    memory limit.# 2) Because of expire: when a key with an associated time to live (see the#    EXPIRE command) must be deleted from memory.# 3) Because of a side effect of a command that stores data on a key that may#    already exist. For example the RENAME command may delete the old key#    content when it is replaced with another one. Similarly SUNIONSTORE#    or SORT with STORE option may delete existing keys. The SET command#    itself removes any old content of the specified key in order to replace#    it with the specified string.# 4) During replication, when a replica performs a full resynchronization with#    its master, the content of the whole database is removed in order to#    load the RDB file just transferred.## In all the above cases the default is to delete objects in a blocking way,# like if DEL was called. However you can configure each case specifically# in order to instead release memory in a non-blocking way like if UNLINK# was called, using the following configuration directives:lazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noreplica-lazy-flush no############################## APPEND ONLY MODE ################################ By default Redis asynchronously dumps the dataset on disk. This mode is# good enough in many applications, but an issue with the Redis process or# a power outage may result into a few minutes of writes lost (depending on# the configured save points).## The Append Only File is an alternative persistence mode that provides# much better durability. For instance using the default data fsync policy# (see later in the config file) Redis can lose just one second of writes in a# dramatic event like a server power outage, or a single write if something# wrong with the Redis process itself happens, but the operating system is# still running correctly.## AOF and RDB persistence can be enabled at the same time without problems.# If the AOF is enabled on startup Redis will load the AOF, that is the file# with the better durability guarantees.## Please check http://redis.io/topics/persistence for more information.appendonly no# The name of the append only file (default: &quot;appendonly.aof&quot;)appendfilename &quot;appendonly.aof&quot;# The fsync() call tells the Operating System to actually write data on disk# instead of waiting for more data in the output buffer. Some OS will really flush# data on disk, some other OS will just try to do it ASAP.## Redis supports three different modes:## no: don&#39;t fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.## The default is &quot;everysec&quot;, as that&#39;s usually the right compromise between# speed and data safety. It&#39;s up to you to understand if you can relax this to# &quot;no&quot; that will let the operating system flush the output buffer when# it wants, for better performances (but if you can live with the idea of# some data loss consider the default persistence mode that&#39;s snapshotting),# or on the contrary, use &quot;always&quot; that&#39;s very slow but a bit safer than# everysec.## More details please check the following article:# http://antirez.com/post/redis-persistence-demystified.html## If unsure, use &quot;everysec&quot;.# appendfsync alwaysappendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background# saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations# Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block# our synchronous write(2) call.## In order to mitigate this problem it&#39;s possible to use the following option# that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.## This means that while another child is saving, the durability of Redis is# the same as &quot;appendfsync none&quot;. In practical terms, this means that it is# possible to lose up to 30 seconds of log in the worst scenario (with the# default Linux settings).## If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as# &quot;no&quot; that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size grows by the specified percentage.## This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (if no rewrite has happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a percentage of zero in order to disable the automatic AOF# rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# An AOF file may be found to be truncated at the end during the Redis# startup process, when the AOF data gets loaded back into memory.# This may happen when the system where Redis is running# crashes, especially when an ext4 filesystem is mounted without the# data=ordered option (however this can&#39;t happen when Redis itself# crashes or aborts but the operating system still works correctly).## Redis can either exit with an error when this happens, or load as much# data as possible (the default now) and start if the AOF file is found# to be truncated at the end. The following option controls this behavior.## If aof-load-truncated is set to yes, a truncated AOF file is loaded and# the Redis server starts emitting a log to inform the user of the event.# Otherwise if the option is set to no, the server aborts with an error# and refuses to start. When the option is set to no, the user requires# to fix the AOF file using the &quot;redis-check-aof&quot; utility before to restart# the server.## Note that if the AOF file will be found to be corrupted in the middle# the server will still exit with an error. This option only applies when# Redis will try to read more data from the AOF file but not enough bytes# will be found.aof-load-truncated yes# When rewriting the AOF file, Redis is able to use an RDB preamble in the# AOF file for faster rewrites and recoveries. When this option is turned# on the rewritten AOF file is composed of two different stanzas:##   [RDB file][AOF tail]## When loading Redis recognizes that the AOF file starts with the &quot;REDIS&quot;# string and loads the prefixed RDB file, and continues loading the AOF# tail.aof-use-rdb-preamble yes################################ LUA SCRIPTING  ################################ Max execution time of a Lua script in milliseconds.## If the maximum execution time is reached Redis will log that a script is# still in execution after the maximum allowed time and will start to# reply to queries with an error.## When a long running script exceeds the maximum execution time only the# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be# used to stop a script that did not yet called write commands. The second# is the only way to shut down the server in the case a write command was# already issued by the script but the user doesn&#39;t want to wait for the natural# termination of the script.## Set it to 0 or a negative value for unlimited execution without warnings.lua-time-limit 5000################################ REDIS CLUSTER  ################################ Normal Redis instances can&#39;t be part of a Redis Cluster; only nodes that are# started as cluster nodes can. In order to start a Redis instance as a# cluster node enable the cluster support uncommenting the following:## cluster-enabled yes# Every cluster node has a cluster configuration file. This file is not# intended to be edited by hand. It is created and updated by Redis nodes.# Every Redis Cluster node requires a different cluster configuration file.# Make sure that instances running in the same system do not have# overlapping cluster configuration file names.## cluster-config-file nodes-6379.conf# Cluster node timeout is the amount of milliseconds a node must be unreachable# for it to be considered in failure state.# Most other internal time limits are multiple of the node timeout.## cluster-node-timeout 15000# A replica of a failing master will avoid to start a failover if its data# looks too old.## There is no simple way for a replica to actually have an exact measure of# its &quot;data age&quot;, so the following two checks are performed:## 1) If there are multiple replicas able to failover, they exchange messages#    in order to try to give an advantage to the replica with the best#    replication offset (more data from the master processed).#    Replicas will try to get their rank by offset, and apply to the start#    of the failover a delay proportional to their rank.## 2) Every single replica computes the time of the last interaction with#    its master. This can be the last ping or command received (if the master#    is still in the &quot;connected&quot; state), or the time that elapsed since the#    disconnection with the master (if the replication link is currently down).#    If the last interaction is too old, the replica will not try to failover#    at all.## The point &quot;2&quot; can be tuned by user. Specifically a replica will not perform# the failover if, since the last interaction with the master, the time# elapsed is greater than:##   (node-timeout * replica-validity-factor) + repl-ping-replica-period## So for example if node-timeout is 30 seconds, and the replica-validity-factor# is 10, and assuming a default repl-ping-replica-period of 10 seconds, the# replica will not try to failover if it was not able to talk with the master# for longer than 310 seconds.## A large replica-validity-factor may allow replicas with too old data to failover# a master, while a too small value may prevent the cluster from being able to# elect a replica at all.## For maximum availability, it is possible to set the replica-validity-factor# to a value of 0, which means, that replicas will always try to failover the# master regardless of the last time they interacted with the master.# (However they&#39;ll always try to apply a delay proportional to their# offset rank).## Zero is the only value able to guarantee that when all the partitions heal# the cluster will always be able to continue.## cluster-replica-validity-factor 10# Cluster replicas are able to migrate to orphaned masters, that are masters# that are left without working replicas. This improves the cluster ability# to resist to failures as otherwise an orphaned master can&#39;t be failed over# in case of failure if it has no working replicas.## Replicas migrate to orphaned masters only if there are still at least a# given number of other working replicas for their old master. This number# is the &quot;migration barrier&quot;. A migration barrier of 1 means that a replica# will migrate only if there is at least 1 other working replica for its master# and so forth. It usually reflects the number of replicas you want for every# master in your cluster.## Default is 1 (replicas migrate only if their masters remain with at least# one replica). To disable migration just set it to a very large value.# A value of 0 can be set but is useful only for debugging and dangerous# in production.## cluster-migration-barrier 1# By default Redis Cluster nodes stop accepting queries if they detect there# is at least an hash slot uncovered (no available node is serving it).# This way if the cluster is partially down (for example a range of hash slots# are no longer covered) all the cluster becomes, eventually, unavailable.# It automatically returns available as soon as all the slots are covered again.## However sometimes you want the subset of the cluster which is working,# to continue to accept queries for the part of the key space that is still# covered. In order to do so, just set the cluster-require-full-coverage# option to no.## cluster-require-full-coverage yes# This option, when set to yes, prevents replicas from trying to failover its# master during master failures. However the master can still perform a# manual failover, if forced to do so.## This is useful in different scenarios, especially in the case of multiple# data center operations, where we want one side to never be promoted if not# in the case of a total DC failure.## cluster-replica-no-failover no# In order to setup your cluster make sure to read the documentation# available at http://redis.io web site.########################## CLUSTER DOCKER/NAT support  ######################### In certain deployments, Redis Cluster nodes address discovery fails, because# addresses are NAT-ted or because ports are forwarded (the typical case is# Docker and other containers).## In order to make Redis Cluster working in such environments, a static# configuration where each node knows its public address is needed. The# following two options are used for this scope, and are:## * cluster-announce-ip# * cluster-announce-port# * cluster-announce-bus-port## Each instruct the node about its address, client port, and cluster message# bus port. The information is then published in the header of the bus packets# so that other nodes will be able to correctly map the address of the node# publishing the information.## If the above options are not used, the normal Redis Cluster auto-detection# will be used instead.## Note that when remapped, the bus port may not be at the fixed offset of# clients port + 10000, so you can specify any port and bus-port depending# on how they get remapped. If the bus-port is not set, a fixed offset of# 10000 will be used as usually.## Example:## cluster-announce-ip 10.1.1.5# cluster-announce-port 6379# cluster-announce-bus-port 6380################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).## You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.slowlog-max-len 128################################ LATENCY MONITOR ############################### The Redis latency monitoring subsystem samples different operations# at runtime in order to collect data related to possible sources of# latency of a Redis instance.## Via the LATENCY command this information is available to the user that can# print graphs and obtain reports.## The system only logs operations that were performed in a time equal or# greater than the amount of milliseconds specified via the# latency-monitor-threshold configuration directive. When its value is set# to zero, the latency monitor is turned off.## By default latency monitoring is disabled since it is mostly not needed# if you don&#39;t have latency issues, and collecting data has a performance# impact, that while very small, can be measured under big load. Latency# monitoring can easily be enabled at runtime using the command# &quot;CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;&quot; if needed.latency-monitor-threshold 0############################# EVENT NOTIFICATION ############################### Redis can notify Pub/Sub clients about events happening in the key space.# This feature is documented at http://redis.io/topics/notifications## For instance if keyspace events notification is enabled, and a client# performs a DEL operation on key &quot;foo&quot; stored in the Database 0, two# messages will be published via Pub/Sub:## PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo## It is possible to select the events that Redis will notify among a set# of classes. Every class is identified by a single character:##  K     Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.#  E     Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.#  g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...#  $     String commands#  l     List commands#  s     Set commands#  h     Hash commands#  z     Sorted set commands#  x     Expired events (events generated every time a key expires)#  e     Evicted events (events generated when a key is evicted for maxmemory)#  A     Alias for g$lshzxe, so that the &quot;AKE&quot; string means all the events.##  The &quot;notify-keyspace-events&quot; takes as argument a string that is composed#  of zero or multiple characters. The empty string means that notifications#  are disabled.##  Example: to enable list and generic events, from the point of view of the#           event name, use:##  notify-keyspace-events Elg##  Example 2: to get the stream of the expired keys subscribing to channel#             name __keyevent@0__:expired use:##  notify-keyspace-events Ex##  By default all notifications are disabled because most users don&#39;t need#  this feature and the feature has some overhead. Note that if you don&#39;t#  specify at least one of K or E, no events will be delivered.notify-keyspace-events &quot;&quot;############################### ADVANCED CONFIG ################################ Hashes are encoded using a memory efficient data structure when they have a# small number of entries, and the biggest entry does not exceed a given# threshold. These thresholds can be configured using the following directives.hash-max-ziplist-entries 512hash-max-ziplist-value 64# Lists are also encoded in a special way to save a lot of space.# The number of entries allowed per internal list node can be specified# as a fixed maximum size or a maximum number of elements.# For a fixed maximum size, use -5 through -1, meaning:# -5: max size: 64 Kb  &lt;-- not recommended for normal workloads# -4: max size: 32 Kb  &lt;-- not recommended# -3: max size: 16 Kb  &lt;-- probably not recommended# -2: max size: 8 Kb   &lt;-- good# -1: max size: 4 Kb   &lt;-- good# Positive numbers mean store up to _exactly_ that number of elements# per list node.# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),# but if your use case is unique, adjust the settings as necessary.list-max-ziplist-size -2# Lists may also be compressed.# Compress depth is the number of quicklist ziplist nodes from *each* side of# the list to *exclude* from compression.  The head and tail of the list# are always uncompressed for fast push/pop operations.  Settings are:# 0: disable all list compression# 1: depth 1 means &quot;don&#39;t start compressing until after 1 node into the list,#    going from either the head or tail&quot;#    So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]#    [head], [tail] will always be uncompressed; inner nodes will compress.# 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]#    2 here means: don&#39;t compress head or head-&gt;next or tail-&gt;prev or tail,#    but compress all nodes between them.# 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]# etc.list-compress-depth 0# Sets have a special encoding in just one case: when a set is composed# of just strings that happen to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the# set in order to use this special memory saving encoding.set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in# order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog sparse representation bytes limit. The limit includes the# 16 bytes header. When an HyperLogLog using the sparse representation crosses# this limit, it is converted into the dense representation.## A value greater than 16000 is totally useless, since at that point the# dense representation is more memory efficient.## The suggested value is ~ 3000 in order to have the benefits of# the space efficient encoding without slowing down too much PFADD,# which is O(N) with the sparse encoding. The value can be raised to# ~ 10000 when CPU is not a concern, but space is, and the data set is# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.hll-sparse-max-bytes 3000# Streams macro node max size / items. The stream data structure is a radix# tree of big nodes that encode multiple items inside. Using this configuration# it is possible to configure how big a single node can be in bytes, and the# maximum number of items it may contain before switching to a new node when# appending new stream entries. If any of the following settings are set to# zero, the limit is ignored, so for instance it is possible to set just a# max entires limit by setting max-bytes to 0 and max-entries to the desired# value.stream-node-max-bytes 4096stream-node-max-entries 100# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation Redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into a hash table# that is rehashing, the more rehashing &quot;steps&quot; are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.## The default is to use this millisecond 10 times every second in order to# actively rehash the main dictionaries, freeing memory when possible.## If unsure:# use &quot;activerehashing no&quot; if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply from time to time# to queries with 2 milliseconds delay.## use &quot;activerehashing yes&quot; if you don&#39;t have such hard requirements but# want to free memory asap when possible.activerehashing yes# The client output buffer limits can be used to force disconnection of clients# that are not reading data from the server fast enough for some reason (a# common reason is that a Pub/Sub client can&#39;t consume messages as fast as the# publisher can produce them).## The limit can be set differently for the three different classes of clients:## normal -&gt; normal clients including MONITOR clients# replica  -&gt; replica clients# pubsub -&gt; clients subscribed to at least one pubsub channel or pattern## The syntax of every client-output-buffer-limit directive is the following:## client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;## A client is immediately disconnected once the hard limit is reached, or if# the soft limit is reached and remains reached for the specified number of# seconds (continuously).# So for instance if the hard limit is 32 megabytes and the soft limit is# 16 megabytes / 10 seconds, the client will get disconnected immediately# if the size of the output buffers reach 32 megabytes, but will also get# disconnected if the client reaches 16 megabytes and continuously overcomes# the limit for 10 seconds.## By default normal clients are not limited because they don&#39;t receive data# without asking (in a push way), but just after a request, so only# asynchronous clients may create a scenario where data is requested faster# than it can read.## Instead there is a default limit for pubsub and replica clients, since# subscribers and replicas receive data in a push fashion.## Both the hard or the soft limit can be disabled by setting them to zero.client-output-buffer-limit normal 0 0 0client-output-buffer-limit replica 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# Client query buffers accumulate new commands. They are limited to a fixed# amount by default in order to avoid that a protocol desynchronization (for# instance due to a bug in the client) will lead to unbound memory usage in# the query buffer. However you can configure it here if you have very special# needs, such us huge multi/exec requests or alike.## client-query-buffer-limit 1gb# In the Redis protocol, bulk requests, that are, elements representing single# strings, are normally limited ot 512 mb. However you can change this limit# here.## proto-max-bulk-len 512mb# Redis calls an internal function to perform many background tasks, like# closing connections of clients in timeout, purging expired keys that are# never requested, and so forth.## Not all tasks are performed with the same frequency, but Redis checks for# tasks to perform according to the specified &quot;hz&quot; value.## By default &quot;hz&quot; is set to 10. Raising the value will use more CPU when# Redis is idle, but at the same time will make Redis more responsive when# there are many keys expiring at the same time, and timeouts may be# handled with more precision.## The range is between 1 and 500, however a value over 100 is usually not# a good idea. Most users should use the default of 10 and raise this up to# 100 only in environments where very low latency is required.hz 10# Normally it is useful to have an HZ value which is proportional to the# number of clients connected. This is useful in order, for instance, to# avoid too many clients are processed for each background task invocation# in order to avoid latency spikes.## Since the default HZ value by default is conservatively set to 10, Redis# offers, and enables by default, the ability to use an adaptive HZ value# which will temporary raise when there are many connected clients.## When dynamic HZ is enabled, the actual configured HZ will be used as# as a baseline, but multiples of the configured HZ value will be actually# used as needed once more clients are connected. In this way an idle# instance will use very little CPU time while a busy instance will be# more responsive.dynamic-hz yes# When a child rewrites the AOF file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.aof-rewrite-incremental-fsync yes# When redis saves RDB file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.rdb-save-incremental-fsync yes# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good# idea to start with the default settings and only change them after investigating# how to improve the performances and how the keys LFU change over time, which# is possible to inspect via the OBJECT FREQ command.## There are two tunable parameters in the Redis LFU implementation: the# counter logarithm factor and the counter decay time. It is important to# understand what the two parameters mean before changing them.## The LFU counter is just 8 bits per key, it&#39;s maximum value is 255, so Redis# uses a probabilistic increment with logarithmic behavior. Given the value# of the old counter, when a key is accessed, the counter is incremented in# this way:## 1. A random number R between 0 and 1 is extracted.# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).# 3. The counter is incremented only if R &lt; P.## The default lfu-log-factor is 10. This is a table of how the frequency# counter changes with a different number of accesses with different# logarithmic factors:## +--------+------------+------------+------------+------------+------------+# | factor | 100 hits   | 1000 hits  | 100K hits  | 1M hits    | 10M hits   |# +--------+------------+------------+------------+------------+------------+# | 0      | 104        | 255        | 255        | 255        | 255        |# +--------+------------+------------+------------+------------+------------+# | 1      | 18         | 49         | 255        | 255        | 255        |# +--------+------------+------------+------------+------------+------------+# | 10     | 10         | 18         | 142        | 255        | 255        |# +--------+------------+------------+------------+------------+------------+# | 100    | 8          | 11         | 49         | 143        | 255        |# +--------+------------+------------+------------+------------+------------+## NOTE: The above table was obtained by running the following commands:##   redis-benchmark -n 1000000 incr foo#   redis-cli object freq foo## NOTE 2: The counter initial value is 5 in order to give new objects a chance# to accumulate hits.## The counter decay time is the time, in minutes, that must elapse in order# for the key counter to be divided by two (or decremented if it has a value# less &lt;= 10).## The default value for the lfu-decay-time is 1. A Special value of 0 means to# decay the counter every time it happens to be scanned.## lfu-log-factor 10# lfu-decay-time 1########################### ACTIVE DEFRAGMENTATION ######################### WARNING THIS FEATURE IS EXPERIMENTAL. However it was stress tested# even in production and manually tested by multiple engineers for some# time.## What is active defragmentation?# -------------------------------## Active (online) defragmentation allows a Redis server to compact the# spaces left between small allocations and deallocations of data in memory,# thus allowing to reclaim back memory.## Fragmentation is a natural process that happens with every allocator (but# less so with Jemalloc, fortunately) and certain workloads. Normally a server# restart is needed in order to lower the fragmentation, or at least to flush# away all the data and create it again. However thanks to this feature# implemented by Oran Agra for Redis 4.0 this process can happen at runtime# in an &quot;hot&quot; way, while the server is running.## Basically when the fragmentation is over a certain level (see the# configuration options below) Redis will start to create new copies of the# values in contiguous memory regions by exploiting certain specific Jemalloc# features (in order to understand if an allocation is causing fragmentation# and to allocate it in a better place), and at the same time, will release the# old copies of the data. This process, repeated incrementally for all the keys# will cause the fragmentation to drop back to normal values.## Important things to understand:## 1. This feature is disabled by default, and only works if you compiled Redis#    to use the copy of Jemalloc we ship with the source code of Redis.#    This is the default with Linux builds.## 2. You never need to enable this feature if you don&#39;t have fragmentation#    issues.## 3. Once you experience fragmentation, you can enable this feature when#    needed with the command &quot;CONFIG SET activedefrag yes&quot;.## The configuration parameters are able to fine tune the behavior of the# defragmentation process. If you are not sure about what they mean it is# a good idea to leave the defaults untouched.# Enabled active defragmentation# activedefrag yes# Minimum amount of fragmentation waste to start active defrag# active-defrag-ignore-bytes 100mb# Minimum percentage of fragmentation to start active defrag# active-defrag-threshold-lower 10# Maximum percentage of fragmentation at which we use maximum effort# active-defrag-threshold-upper 100# Minimal effort for defrag in CPU percentage# active-defrag-cycle-min 5# Maximal effort for defrag in CPU percentage# active-defrag-cycle-max 75# Maximum number of set/hash/zset/list fields that will be processed from# the main dictionary scan# active-defrag-max-scan-fields 1000</code></pre><p><strong>常用的参数</strong><br><a href="https://www.runoob.com/redis/redis-conf.html" target="_blank" rel="noopener">点击查看常用的配置参数资料</a></p><h3 id="Redis基本测试redis-benchmark"><a href="#Redis基本测试redis-benchmark" class="headerlink" title="Redis基本测试redis-benchmark"></a><code>Redis</code>基本测试<code>redis-benchmark</code></h3><pre><code class="bash">1    -h    指定服务器主机名    127.0.0.12    -p    指定服务器端口    63793    -s    指定服务器 socket    4    -c    指定并发连接数    505    -n    指定请求数    100006    -d    以字节的形式指定 SET/GET 值的数据大小    27    -k    1=keep alive 0=reconnect    18    -r    SET/GET/INCR 使用随机 key, SADD 使用随机值    9    -P    通过管道传输 &lt;numreq&gt; 请求    110    -q    强制退出 redis。仅显示 query/sec 值    11    --csv    以 CSV 格式输出    12    -l    生成循环，永久执行测试    13    -t    仅运行以逗号分隔的测试命令列表。    14    -I    Idle 模式。仅打开 N 个 idle 连接并等待。    </code></pre><h3 id="性能测试实例"><a href="#性能测试实例" class="headerlink" title="性能测试实例"></a>性能测试实例</h3><p><strong>以下实例我们使用了多个参数来测试 redis 性能：</strong></p><pre><code class="bash">$ redis-benchmark -h 127.0.0.1 -p 6379 -t set,lpush -n 10000 -qSET: 146198.83 requests per secondLPUSH: 145560.41 requests per second</code></pre><h3 id="常用的命令📓"><a href="#常用的命令📓" class="headerlink" title="常用的命令📓"></a>常用的命令📓</h3><table><thead><tr><th>命令</th><th>作用</th></tr></thead><tbody><tr><td><code>SELECT n</code></td><td>选择数据库</td></tr><tr><td><code>KEYS *</code></td><td>查看KEY列表</td></tr><tr><td><code>DEL KEY</code></td><td>删除指定KEY</td></tr><tr><td><code>TYPE KEY</code></td><td>查看KEY类型</td></tr><tr><td><code>SET KEY VALUE</code></td><td>设置KEY和值</td></tr><tr><td><code>GET KEY</code></td><td>获取KEY的值</td></tr><tr><td><code>DBSZIE</code></td><td>查看DB大小</td></tr><tr><td><code>FLUSHDB</code></td><td>清空当前DB</td></tr><tr><td><code>FLUSHALL</code></td><td>清空所有DB</td></tr><tr><td><code>FLUSHALL</code></td><td>清空所有DB</td></tr><tr><td><code>EXPIRE KEY S</code></td><td>设置KEY多久到期/秒</td></tr><tr><td><code>TTL KEY</code></td><td>查看KEY还有多久到期</td></tr></tbody></table>]]></content>
    
    
    
    <tags>
      
      <tag>Go</tag>
      
      <tag>Redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>浅谈分布式消息队列</title>
    <link href="/2020/05/27/%E6%B5%85%E8%B0%88%E5%88%86%E5%B8%83%E5%BC%8F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    <url>/2020/05/27/%E6%B5%85%E8%B0%88%E5%88%86%E5%B8%83%E5%BC%8F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/</url>
    
    <content type="html"><![CDATA[<h2 id="什么消息队列"><a href="#什么消息队列" class="headerlink" title="什么消息队列?"></a>什么消息队列?</h2><p><code>消息队列</code>这个词陌生人一听挖槽好牛逼啊好高大上啊，消息队列，一般我们会简称它为<code>MQ(Message Queue)</code>，嗯，就是很直白的简写。</p><ul><li>队列是一种先进先出的数据结构</li><li>消息队列<code>（MessageQueue）</code>是一种应用间的通信方式</li><li>消息可以非常简单，比如只包含文本字符串，也可以更复杂，可能包含嵌入对象</li><li>消息发送者 可以发送一个消息而无须等待响应。消息发送者 将消息发送到一条 虚拟的通道（主题 或 队列）上，消息接收者 则 订阅 或是 监听 该通道。一条信息可能最终转发给 一个或多个 消息接收者，这些接收者都无需对 消息发送者 做出 同步回应。整个过程都是 异步的。</li></ul><blockquote><p>维基百科上是这么描述的: 在计算机科学中，消息队列是一种进程间通信或同一进程的不同线程间的通信方式，软件的贮列用来处理一系列的输入，通常是来自用户。消息队列提供了异步的通信协议，每一个贮列中的纪录包含详细说明的数据，包含发生的时间，输入设备的种类，以及特定的输入参数，也就是说：消息的发送者和接收者不需要同时与消息队列交互。</p></blockquote><p><strong>我<code>SDing</code>个人总结就是: 消息队列是一个先进先出的数据存储容器，它可以帮助不同的程序之间进行数据的通讯。</strong></p><h2 id="发布-订阅"><a href="#发布-订阅" class="headerlink" title="发布/订阅"></a>发布/订阅</h2><p>说到消息队列就要说到消息队列里面最常用的方式就是发布和订阅了。</p><blockquote><p>维基百科上是这么描述的: 在软件架构中，发布-订阅是一种消息范式，消息的发送者不会将消息直接发送给特定的接收者。而是将发布的消息分为不同的类别，无需了解哪些订阅者可能存在。同样的，订阅者可以表达对一个或多个类别的兴趣，只接收感兴趣的消息，无需了解哪些发布者存在。 发布/订阅是消息队列范式的兄弟，通常是更大的面向消息中间件系统的一部分。 </p></blockquote><ul><li>消息生产者 （生产消息发送到消息队列里）</li><li>消息中间件 <code>(Kafka,Nsq,RabbitMQ)</code></li><li>消息消费者  (从消息队列里面拿取订阅的消息)</li></ul><p><strong>我的一句话总结就是: 消息发布和订阅就是一个架构方式，可以在不同的程序之间获取自己需要的数据，解耦模式。</strong><br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7dwe18p4j30hj07vaaq.jpg" srcset="/img/loading.gif" alt="消息队列"></p><p>可以看看这个倔金的文章:<a href="https://juejin.im/post/5cb025fb5188251b0351ef48" target="_blank" rel="noopener">消息队列解耦</a> or <a href="https://juejin.im/post/5b41fe36e51d45191252e79e" target="_blank" rel="noopener">消息队列详解</a></p><h2 id="常见中间件"><a href="#常见中间件" class="headerlink" title="常见中间件"></a>常见中间件</h2><ol><li>NSQ</li><li>Kafka</li><li>ZeroMQ</li><li>MetaMQ</li><li>RabbitMQ</li><li>RocketMQ</li><li>ActiveMQ</li><li>Redis (主要是缓存但是也可以实现消息队列)</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>Go</tag>
      
      <tag>消息队列</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Code-Server搭建</title>
    <link href="/2020/05/26/Code-Server%E6%90%AD%E5%BB%BA/"/>
    <url>/2020/05/26/Code-Server%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf5z665ly1j30dc06o74k.jpg" srcset="/img/loading.gif" alt=""></p><blockquote><p>在我们开发调试程序的时候我们的一些程序部署在服务器上运行，但是很多配置文件，这样我们修改起来比较麻烦，使用我们可以通过vscode服务器版本进行开发。</p></blockquote><h2 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h2><p>安装可以通过这个<code>https://github.com/cdr/code-server</code>仓库上面的文件来。</p><p><strong>根据自己对应的系统版本安装</strong></p><h3 id="Debian-Ubuntu"><a href="#Debian-Ubuntu" class="headerlink" title="Debian, Ubuntu"></a>Debian, Ubuntu</h3><pre><code class="shell">curl -sSOL https://github.com/cdr/code-server/releases/download/v3.3.1/code-server_3.3.1_amd64.debsudo dpkg -i code-server_3.3.1_amd64.debsystemctl --user enable --now code-server# Now visit http://127.0.0.1:8080. Your password is in ~/.config/code-server/config.yaml</code></pre><h3 id="Fedora-Red-Hat-SUSE"><a href="#Fedora-Red-Hat-SUSE" class="headerlink" title="Fedora, Red Hat, SUSE"></a>Fedora, Red Hat, SUSE</h3><pre><code class="shell">curl -sSOL https://github.com/cdr/code-server/releases/download/v3.3.1/code-server-3.3.1-amd64.rpmsudo yum install -y code-server-3.3.1-amd64.rpmsystemctl --user enable --now code-server# Now visit http://127.0.0.1:8080. Your password is in ~/.config/code-server/config.yaml</code></pre><h3 id="Arch-Linux"><a href="#Arch-Linux" class="headerlink" title="Arch Linux"></a>Arch Linux</h3><pre><code class="shell"># Installs code-server from the AUR using yay.yay -S code-serversystemctl --user enable --now code-server# Now visit http://127.0.0.1:8080. Your password is in ~/.config/code-server/config.yaml# Installs code-server from the AUR with plain makepkg.git clone https://aur.archlinux.org/code-server.gitcd code-servermakepkg -sisystemctl --user enable --now code-server</code></pre><p>默认访问地址: <code>http://127.0.0.1:8080</code>. 默认密码配置:<code>~/.config/code-server/config.yaml</code></p><h3 id="Firewall-wall-setting-port"><a href="#Firewall-wall-setting-port" class="headerlink" title="Firewall wall setting port"></a>Firewall wall setting port</h3><p>  <code>–permanent</code> 添加这个参数防火墙永久生效. </p><p>  默认端口是 8080.</p><pre><code>$: firewall-cmd --zone=public --add-port=8080/tcp --permanent# 刷新配置$: firewall-cmd --reload</code></pre><h3 id="Edit-config-file"><a href="#Edit-config-file" class="headerlink" title="Edit config file"></a>Edit config file</h3><p>  使用 <code>vi</code> 打开配置文件<code>~/.config/code-server/config.yaml</code></p><pre><code>$: vi  ~/.config/code-server/config.yaml</code></pre><p>  使用键盘<code>i</code> 键进入insert模式</p><pre><code> bind-addr: 127.0.0.1:8080        auth: password password: 706039b9fa******b847cec0c cert: false ~ ~ -- INSERT --</code></pre><p>   <code>bind-addr</code> 修改为 <code>0.0.0.0:8080</code> (这样就可以远程访问)<br>   <code>password</code> 修改为你自定义密码</p><p>  按键盘上的<code>esc</code> 键输入<code>: wq</code> 保存退出文件</p><h3 id="启动服务器"><a href="#启动服务器" class="headerlink" title="启动服务器"></a>启动服务器</h3><pre><code>$: /usr/bin/code-server</code></pre><p>output:</p><pre><code>[root@vultr ~]# /usr/bin/code-serverinfo  Wrote default config file to ~/.config/code-server/config.yamlinfo  Using config file ~/.config/code-server/config.yamlinfo  Using user-data-dir ~/.local/share/code-serverinfo  code-server 3.3.1 6f1309795e1cb930edba68cdc7c3dcaa01da0ab3info  HTTP server listening on http://127.0.0.1:8080info      - Using password from ~/.config/code-server/config.yamlinfo      - To disable use `--auth none`info    - Not serving HTTPS</code></pre><p><strong>不需要密码可以添加 <code>-auth none</code> 参数</strong></p><p> <strong>or  start by designated user</strong></p><pre><code>$:  systemctl --user enable --now code-server</code></pre><p> <strong>now visit <a href="http://your.server.address:8080" target="_blank" rel="noopener">http://your.server.address:8080</a>. Good luck~</strong></p>]]></content>
    
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>vscode</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Azure上安装Kafka</title>
    <link href="/2020/05/25/Azure%E4%B8%8A%E5%AE%89%E8%A3%85Kafka/"/>
    <url>/2020/05/25/Azure%E4%B8%8A%E5%AE%89%E8%A3%85Kafka/</url>
    
    <content type="html"><![CDATA[<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf4liai3d3j30k008jt9d.jpg" srcset="/img/loading.gif" alt=""></p><h2 id="什么是Kafka"><a href="#什么是Kafka" class="headerlink" title="什么是Kafka?"></a>什么是Kafka?</h2><p>Kafka 是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者在网站中的所有动作流数据。 这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。 这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。<br>对于像 Hadoop 一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka 的目的是通过 Hadoop 的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消息。<br>Kafka 是由 Apache 软件基金会开发的一个开源流处理平台，由 Scala 和 Java 编写。<br>Kafka 最初是由领英开发，并随后于 2011 年初开源，并于 2012 年 10 月 23 日由 Apache Incubator 孵化出站。2014 年 11 月，几个曾在领英为 Kafka 工作的工程师，创建了名为 Confluent 的新公司，并着眼于 Kafka。根据 2014 年 Quora 的帖子，Jay Kreps 似乎已经将它以作家弗朗茨·卡夫卡命名。Kreps 选择将该系统以一个作家命名是因为，它是“一个用于优化写作的系统”，而且他很喜欢卡夫卡的作品。</p><p><strong>Kafka 存储的消息来自任意多被称为“生产者”（Producer）的进程。数据从而可以被分配到不同的“分区”（Partition）、不同的“Topic”下。在一个分区内，这些消息被索引并连同时间戳存储在一起。其它被称为“消费者”（Consumer）的进程可以从分区查询消息。Kafka 运行在一个由一台或多台服务器组成的集群上，并且分区可以跨集群结点分布。</strong></p><p><strong>Kafka 高效地处理实时流式数据，可以实现与 Storm、HBase 和 Spark 的集成。作为聚类部署到多台服务器上，Kafka 处理它所有的发布和订阅消息系统使用了四个 API，即生产者 API、消费者 API、Stream API 和 Connector API。它能够传递大规模流式消息，自带容错功能，已经取代了一些传统消息系统，如 JMS、AMQP 等。</strong></p><p><strong>Kafka 架构的主要术语包括 Topic、Record 和 Broker。Topic 由 Record 组成，Record 持有不同的信息，而 Broker 则负责复制消息。Kafka 有四个主要 API：</strong></p><ul><li>生产者 API：支持应用程序发布 Record 流。</li><li>消费者 API：支持应用程序订阅 Topic 和处理 Record 流。</li><li>Stream API：将输入流转换为输出流，并产生结果。</li><li>Connector API：执行可重用的生产者和消费者 API，可将 Topic 链接到现有应用程序。</li></ul><p><strong>相关术语</strong></p><ul><li>Topic 用来对消息进行分类，每个进入到 Kafka 的信息都会被放到一个 Topic 下</li><li>Broker 用来实现数据存储的主机服务器</li><li>Partition 每个 Topic 中的消息会被分为若干个 Partition，以提高消息的处理效率</li><li>Producer 消息的生产者</li><li>Consumer 消息的消费者</li><li>Consumer Group 消息的消费群组</li></ul><p>由于其广泛集成到企业级基础设施中，监测 Kafka 在规模运行中的性能成为一个日益重要的问题。监测端到端性能，要求跟踪所有指标，包括 Broker、消费者和生产者。除此之外还要监测 ZooKeeper，Kafka 用它来协调各个消费者。当前有一些监测平台可以跟踪卡夫卡的性能，有开源的，如领英的 Burrow；也有付费的，如 Datadog。除了这些平台之外，收集 Kafka 的数据也可以使用工具来进行，这些工具一般需要 Java，包括 JConsole。</p><h2 id="Get-Started"><a href="#Get-Started" class="headerlink" title="Get Started"></a>Get Started</h2><p>Kafaka是基于是JVM运行的程序，所有我们需要先安装JDK或者JRE。<code>(这里就看到了我们的Golang优势！！！！直接编译二进制啊哈哈哈~)</code></p><p><strong>需要Java 1.7或更高版本。因此，您可以如下安装OpenJDK 8</strong></p><pre><code class="shell">sudo yum install -y java-1.13.0-openjdk-devel</code></pre><p><strong>安装了OpenJDK 8后，请使用以下命令验证结果。</strong></p><pre><code class="shell">java -version</code></pre><p><strong>如果一切顺利，输出应与此类似。</strong></p><pre><code class="shelL">openjdk version &quot;1.8.0_252&quot;OpenJDK Runtime Environment (build 1.8.0_252-b09)OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)</code></pre><p><strong>最后，设置JAVA_HOME环境变量。</strong></p><pre><code class="shell">echo &quot;JAVA_HOME=$(readlink -f /usr/bin/java | sed &quot;s:bin/java::&quot;)&quot; | sudo tee -a /etc/profilesource /etc/profile</code></pre><p><strong>安装Kafka</strong></p><pre><code class="shell">wget https://downloads.apache.org/kafka/2.4.1/kafka_2.12-2.4.1.tgz</code></pre><p><strong>解压到opt目录</strong></p><pre><code class="SHELL"> tar -xvf kafka_2.12-2.4.1.tgz -C /opt/</code></pre><p>进入Kafka目录</p><p><code>cd /opt/kafka_2.11-0.9.0.1</code><br>启动Zookeeper服务器</p><p><code>bin/zookeeper-server-start.sh -daemon config/zookeeper.properties</code></p><p>修改Kafka服务器的配置</p><p> <code>vi bin/kafka-server-start.sh</code></p><p>根据您的特定系统参数调整内存使用量。</p><p><code>export KAFKA_HEAP_OPTS=&quot;-Xmx1G -Xms1G&quot;</code></p><p>替换为：</p><p><code>export KAFKA_HEAP_OPTS=&quot;-Xmx256M -Xms128M&quot;</code></p><p>保存退出：</p><p><code>:wq</code></p><p>启动Kafka服务器</p><p><code>bin/kafka-server-start.sh config/server.properties</code></p><p>如果一切顺利，您将看到一些有关Kafka服务器状态的消息，最后一条将显示：</p><p><code>INFO [Kafka Server 0], started (kafka.server.KafkaServer)</code></p><p>这意味着您已经启动了Kafka服务器。</p><p>在新的SSH连接中创建主题“测试”</p><p>打开一个新的SSH连接，使用以下命令创建主题“ test”：</p><pre><code class="shell">cd /opt/kafka_2.11-0.9.0.1bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test</code></pre><p>您可以使用以下命令查看主题：</p><pre><code class="shell">bin/kafka-topics.sh --list --zookeeper localhost:2181</code></pre><p>在我们的例子中，输出将显示为：</p><pre><code class="shell">test</code></pre><p>使用主题“测试”生成消息</p><pre><code class="shell">bin/kafka-console-producer.sh --broker-list localhost:9092 </code></pre><p><code>--topic test</code><br>使用上面的命令，您可以根据需要输入任意数量的消息，例如：</p><pre><code class="shell">Welcome aboard!Bonjour!</code></pre><p>如果您收到与”WARN Error while fetching metadata with correlation id”输入消息时类似的错误，则需要server.properties使用以下信息更新文件：</p><pre><code class="shell">port = 9092advertised.host.name = localhost </code></pre><p>打开第三个SSH连接，然后运行以下命令：</p><pre><code class="shell">cd /opt/kafka_2.11-0.9.0.1bin/kafka-console-consumer.sh --zookeeper localhost:9092 --topic test --from-beginning</code></pre><ul><li>您先前生成的消息将显示在第三个SSH连接中。当然，如果您现在从第二个SSH连接输入更多消息，您将立即在第三个SSH连接上看到它们。</li></ul><p>最后，您可以在每个SSH连接上按Ctrl + C组合键以停止这些脚本。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Centos7</tag>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NSQ之弹射起步</title>
    <link href="/2020/05/19/NSQ%E4%B9%8B%E5%BC%B9%E5%B0%84%E8%B5%B7%E6%AD%A5/"/>
    <url>/2020/05/19/NSQ%E4%B9%8B%E5%BC%B9%E5%B0%84%E8%B5%B7%E6%AD%A5/</url>
    
    <content type="html"><![CDATA[<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gey1a5ssbvj30p00dw77v.jpg" srcset="/img/loading.gif" alt=""></p><h2 id="NSQ是什么？？？"><a href="#NSQ是什么？？？" class="headerlink" title="NSQ是什么？？？"></a><code>NSQ</code>是什么？？？</h2><p><strong><code>NSQ</code>是<code>Go</code>语言编写的一个开源的实时分布式内存消息队列，其性能十分优异。 NSQ的优势有以下优势：</strong></p><ul><li><code>NSQ</code>提倡分布式和分散的拓扑，没有单点故障，支持容错和高可用性，并提供可靠的消息交付保证</li><li><code>NSQ</code>支持横向扩展，没有任何集中式代理。</li><li><code>NSQ</code>易于配置和部署，并且内置了管理界面。</li></ul><h2 id="什么是消息队列"><a href="#什么是消息队列" class="headerlink" title="什么是消息队列?"></a>什么是消息队列?</h2><p>通俗地解释一下消息队列，你想象一个场景：你到报社订阅了一份报纸，报社每日生产一份新报纸，便将新报纸发往邮局并告诉邮局你的地址，邮递员将你的报纸送往你的邮箱，你便可以愉快地阅读今天的时事新闻了。当然，可能一个人订阅了好几家报社，一家报社也可以被多个人订阅。在这个场景中，消息队列就担任了，邮箱、邮局、邮递员的角色。</p><h2 id="消息队列的作用？"><a href="#消息队列的作用？" class="headerlink" title="消息队列的作用？"></a>消息队列的作用？</h2><ul><li>是减少相应所需的时间和削峰</li><li>降低系统耦合性解耦或提升系统的可拓展性）</li></ul><p><strong>如图下场景:</strong><br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gextpsdc8qj313e0u0wja.jpg" srcset="/img/loading.gif" alt="nsq"><br><strong>通过使用消息队列将不同的业务逻辑解耦，降低系统间的耦合，提高系统的健壮性。后续有其他业务要使用订单数据可直接订阅消息队列，提高系统的灵活性。</strong><br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gextqm9slzj30wi0jg409.jpg" srcset="/img/loading.gif" alt="nsq"><br><strong>还可以帮大并发做流量抗压处理，类似秒杀（大秒）等场景下，某一时间可能会产生大量的请求，使用消息队列能够为后端处理请求提供一定的缓冲区，保证后端服务的稳定性。</strong><br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gextrsvl7uj30xc08uwf9.jpg" srcset="/img/loading.gif" alt="nsq"></p><h2 id="开始NSQ弹射起步🚀"><a href="#开始NSQ弹射起步🚀" class="headerlink" title="开始NSQ弹射起步🚀"></a>开始<code>NSQ</code>弹射起步🚀</h2><p><strong>NSQ工作模式👇</strong><br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gexu2qzvroj31480u0792.jpg" srcset="/img/loading.gif" alt="nsq"></p><p><code>Topic</code>和<code>Channel</code><br>每个<code>nsqd</code>实例旨在一次处理多个数据流。这些数据流称为<code>“topics”</code>，一个<code>topic</code>具有1个或多个<code>“channels”</code>。每个<code>channel</code>都会收到<code>topic</code>所有消息的副本，实际上下游的服务是通过对应的<code>channel</code>来消费<code>topic</code>消息。</p><p><code>topic</code>和<code>channel</code>不是预先配置的。<code>topic</code>在首次使用时创建，方法是将其发布到指定<code>topic</code>，或者订阅指定<code>topic</code>上的<code>channel</code>。<code>channel</code>是通过订阅指定的<code>channel</code>在第一次使用时创建的。</p><p><code>topic</code>和<code>channel</code>都相互独立地缓冲数据，防止缓慢的消费者导致其他<code>chennel</code>的积压（同样适用于<code>topic</code>级别）。</p><p><code>channel</code>可以并且通常会连接多个客户端。假设所有连接的客户端都处于准备接收消息的状态，则每条消息将被传递到随机客户端。例如：<br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gexw4g26pog30bo07tjry.gif" srcset="/img/loading.gif" alt="nsq"><br>总而言之，消息是从<code>topic -&gt; channel</code>（每个<code>channel</code>接收该<code>topic</code>的所有消息的副本）多播的，但是从<code>channel -&gt; consumers</code>均匀分布（每个消费者接收该<code>channel</code>的一部分消息）。<br><strong>NSQ接收和发送消息流程</strong><br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gexu6v1zghj30fc07paah.jpg" srcset="/img/loading.gif" alt=""><br><strong>NSQ特性</strong><br>消息默认不持久化，可以配置成持久化模式。nsq采用的方式时内存+硬盘的模式，当内存到达一定程度时就会将数据持久化到硬盘。</p><ul><li>如果将<code>--mem-queue-size</code>设置为0，所有的消息将会存储到磁盘。</li><li>服务器重启时也会将当时在内存中的消息持久化。</li><li>每条消息至少传递一次。</li><li>消息不保证有序。</li></ul><h3 id="开始使用NSQ"><a href="#开始使用NSQ" class="headerlink" title="开始使用NSQ"></a>开始使用<code>NSQ</code></h3><p>首先我们需要安装NSQ的<code>nsqlookupd</code>组件也就是<code>nsqd</code>程序,浏览器打开<a href="https://nsq.io/deployment/installing.html" target="_blank" rel="noopener">官方下载页面</a>即可得到你对应操作系统的程序。<br><strong><code>nsqd</code>是一个守护进程，它接收、排队并向客户端发送消息。</strong></p><h3 id="nsqlookupd组件"><a href="#nsqlookupd组件" class="headerlink" title="nsqlookupd组件"></a><code>nsqlookupd</code>组件</h3><p><code>nsqlookupd</code>相当于集群的管理员所以我们首先启动它</p><pre><code class="shell">$ nsqlookupd</code></pre><h3 id="nsqd组件"><a href="#nsqd组件" class="headerlink" title="nsqd组件"></a><code>nsqd</code>组件</h3><p>启动 <code>nsqd</code>，它相当于集群下的节点，是主要干活的</p><pre><code class="shell">$ nsqd --lookupd-tcp-address=127.0.0.1:4160</code></pre><p><strong><code>nsqlookupd</code>是维护所有<code>nsqd</code>状态、提供服务发现的守护进程。它能为消费者查找特定<code>topic</code>下的<code>nsqd</code>提供了运行时的自动发现服务。 它不维持持久状态，也不需要与任何其他<code>nsqlookupd</code>实例协调以满足查询。因此根据你系统的冗余要求尽可能多地部署<code>nsqlookupd</code>节点。它们消耗的资源很少，可以与其他服务共存。我们的建议是为每个数据中心运行至少3个集群。</strong></p><h3 id="nsqadmin组件"><a href="#nsqadmin组件" class="headerlink" title="nsqadmin组件"></a><code>nsqadmin</code>组件</h3><p>一个实时监控集群状态、执行各种管理任务的Web管理平台。 启动<code>nsqadmin</code>，指定<code>nsqlookupd</code>地址:</p><pre><code class="shell"> ./nsqadmin -lookupd-http-address=127.0.0.1:4161</code></pre><p>我们可以使用浏览器打开<code>http://127.0.0.1:4171/</code>访问如下管理界面。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gexvk9mzt7j31c00u0b29.jpg" srcset="/img/loading.gif" alt="nsq"></p><h3 id="测试消息"><a href="#测试消息" class="headerlink" title="测试消息"></a>测试消息</h3><p>发送一条消息</p><pre><code class="shell">$ curl -d &#39;hello world 1&#39; &#39;http://127.0.0.1:4151/pub?topic=test&#39;</code></pre><p>即可看到收到了消息<br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gexvqdbqrrj31740u0k9x.jpg" srcset="/img/loading.gif" alt="nsq"><br>把消息写到硬盘的文件里</p><pre><code class="shell">$ nsq_to_file --topic=test --output-dir=/tmp --lookupd-http-address=127.0.0.1:4161</code></pre><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gexvtcplxej312i0u07c5.jpg" srcset="/img/loading.gif" alt="nsq"><br>文件内容<br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gexvukbk33j31ja0m614z.jpg" srcset="/img/loading.gif" alt="nsq"></p><p><strong>好了如果对你有帮助你可以到本博客的关于页面添加我微信或者在Github联系我，一起推动Golang在中国这片土地生根发芽长成大树！！！Good luck!</strong></p>]]></content>
    
    
    
    <tags>
      
      <tag>Go</tag>
      
      <tag>NSQ</tag>
      
      <tag>消息队列</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Gohper成长秘籍</title>
    <link href="/2020/05/17/Gohper%E7%9A%84300%E9%97%AE/"/>
    <url>/2020/05/17/Gohper%E7%9A%84300%E9%97%AE/</url>
    
    <content type="html"><![CDATA[<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gevv4s06cej30dw04zjs2.jpg" srcset="/img/loading.gif" alt=""></p><blockquote><p>这是我个人<code>Denn</code>整理的一套Golang学习目录<br> ✅ 表示我已经完成的<br> 🤷‍♀️ 表示我了解过没有深入<br> 加油各位少年🍻</p></blockquote><h1 id="目录树🌲"><a href="#目录树🌲" class="headerlink" title="目录树🌲"></a>目录树🌲</h1><ul><li>常量和变量的关系 ✅</li><li>基本数据类型和引用类型  ✅</li><li>程序的运算符  ✅</li><li>程序控制流程  ✅</li><li>数组的使用  ✅</li><li><code>slice</code>、<code>map</code>、<code>channel</code>的使用  ✅</li><li>函数式编程  ✅</li><li>结构体和指针  ✅</li><li>面向接口编程  ✅</li><li>包和目录的概念  ✅</li><li>反射和不安全编程 🤷‍♀️</li><li>测试包的使用  ✅</li><li>常用的内置的使用  ✅</li><li><code>Goroutine</code>和<code>Comtext</code>  ✅</li><li>网络编程和数据库编程  ✅<ul><li><code>Gin</code></li><li><code>Gorm</code></li><li><code>gRPC</code> </li><li><code>Redis</code></li><li><code>Nsq</code></li><li><code>Kafka</code></li><li><code>MongoDB</code></li><li><code>Etcd</code></li><li><code>ElasticSearch</code></li><li><code>......还有跟多需要不断努力🏃‍♀️</code></li></ul></li><li>算法和数据结构补一补</li><li>找一些计算机组成和计算机基础视频看看~</li><li>Good Luck！！！</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Go</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Go性能分析之PProf</title>
    <link href="/2020/05/16/Go%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B9%8BPProf/"/>
    <url>/2020/05/16/Go%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B9%8BPProf/</url>
    
    <content type="html"><![CDATA[<h2 id="PProf是个啥？？？"><a href="#PProf是个啥？？？" class="headerlink" title="PProf是个啥？？？"></a>PProf是个啥？？？</h2><p><code>pprof</code>是用于可视化和分析性能分析数据的工具,<code>pprof</code>以 <code>profile.proto</code> 读取分析样本的集合，并生成报告以可视化并帮助分析数据（支持文本和图形报告）<br><code>profile.proto</code> 是一个<code>Protocol Buffer v3</code>的描述文件，它描述了一组 <code>callstack</code>和 <code>symbolization</code>信息， 作用是表示统计分析的一组采样的调用栈，是很常见的 <code>stacktrace</code>配置文件格式。</p><ol><li>支持什么使用模式<ul><li><code>Report generation</code>：报告生成</li><li><code>Interactive terminal use</code>：交互式终端使用</li><li><code>Web interface</code>：<code>Web</code> 界面</li></ul></li><li>可以做什么<ul><li><code>CPU Profiling</code>：<code>CPU</code> 分析，按照一定的频率采集所监听的应用程序 <code>CPU</code>（含寄存器）的使用情况，可确定应用程序在主动消耗 <code>CPU</code> 周期时花费时间的位置</li><li><code>Memory Profiling</code>：内存分析，在应用程序进行堆分配时记录堆栈跟踪，用于监视当前和历史内存使用情况，以及检查内存泄漏</li><li><code>Block Profiling</code>：阻塞分析，记录 <code>goroutine</code> 阻塞等待同步（包括定时器通道）的位置</li><li><code>Mutex Profiling</code>：互斥锁分析，报告互斥锁的竞争情况</li></ul></li></ol><h2 id="一个例子🌰"><a href="#一个例子🌰" class="headerlink" title="一个例子🌰"></a>一个例子🌰</h2><p><strong>想要进行性能优化，首先瞩目在 Go 自身提供的工具链来作为分析依据，本文将带你学习、使用 Go 后花园，涉及如下：</strong></p><ul><li><code>runtime/pprof</code>：采集程序（<code>非 Server</code>）的运行数据进行分析</li><li><code>net/http/pprof</code>：采集 <code>HTTP Server</code> 的运行时数据进行分析</li></ul><pre><code class="go">// Copyright (c) 2020 HigKer// Open Source: MIT License// Author: SDing &lt;deen.job@qq.com&gt;// Date: 2020/5/16 - 12:53 下午package mainimport (    &quot;flag&quot;    &quot;fmt&quot;    &quot;os&quot;    &quot;runtime/pprof&quot;    &quot;time&quot;)// 一段有问题的代码func logicCode() {    var c chan int    for {        select {        case v := &lt;-c:            fmt.Printf(&quot;recv from chan, value:%v\n&quot;, v)        default:        }    }}func main() {    var isCPUPprof bool    var isMemPprof bool    flag.BoolVar(&amp;isCPUPprof, &quot;cpu&quot;, false, &quot;turn cpu pprof on&quot;)    flag.BoolVar(&amp;isMemPprof, &quot;mem&quot;, false, &quot;turn mem pprof on&quot;)    flag.Parse()    if isCPUPprof {         file, err := os.Create(&quot;./cpu.pprof&quot;)        if err != nil {            fmt.Printf(&quot;create cpu pprof failed, err:%v\n&quot;, err)            return        }        pprof.StartCPUProfile(file)        defer pprof.StopCPUProfile()    }    for i := 0; i &lt; 8; i++ {        go logicCode()    }    time.Sleep(20 * time.Second)    if isMemPprof {        file, err := os.Create(&quot;./mem.pprof&quot;)        if err != nil {            fmt.Printf(&quot;create mem pprof failed, err:%v\n&quot;, err)            return        }        pprof.WriteHeapProfile(file)        file.Close()    }}</code></pre><p><strong>可以看到👆上面的那个<code>select</code>代码块里面的channel没有初始化，会一直阻塞着。这样一个简单的代码可以通过我们肉眼出来了，但是如果是复杂的我们就需要使用<code>pprof</code>工具了。</strong></p><pre><code class="shell">$: go tool pprof cpu.pprof Type: cpuTime: May 16, 2020 at 12:59pm (CST)Duration: 20.14s, Total samples = 56.90s (282.48%)Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)(pprof) top3Showing nodes accounting for 51.91s, 91.23% of 56.90s totalDropped 7 nodes (cum &lt;= 0.28s)Showing top 3 nodes out of 4      flat  flat%   sum%        cum   cum%    23.04s 40.49% 40.49%     45.18s 79.40%  runtime.selectnbrecv    17.26s 30.33% 70.83%     18.93s 33.27%  runtime.chanrecv    11.61s 20.40% 91.23%     56.83s 99.88%  main.logicCode //❎这里已经通过工具分析出来了！！！</code></pre><p><strong>工具就可以分析出来了我们的<code>logicCode</code>函数有问题！</strong></p><pre><code class="shell">$: go tool pprof cpu.pprof Type: cpuTime: May 16, 2020 at 12:59pm (CST)Duration: 20.14s, Total samples = 56.90s (282.48%)Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)(pprof) top3Showing nodes accounting for 51.91s, 91.23% of 56.90s totalDropped 7 nodes (cum &lt;= 0.28s)Showing top 3 nodes out of 4      flat  flat%   sum%        cum   cum%    23.04s 40.49% 40.49%     45.18s 79.40%  runtime.selectnbrecv    17.26s 30.33% 70.83%     18.93s 33.27%  runtime.chanrecv    11.61s 20.40% 91.23%     56.83s 99.88%  main.logicCode(pprof) list logicCodeTotal: 56.90sROUTINE ======================== main.logicCode in /Users/ding/Documents/GO_CODE_DEV/src/Lets_Go/lets_37_pprof/main.go    11.61s     56.83s (flat, cum) 99.88% of Total         .          .     16:// 一段有问题的代码         .          .     17:func logicCode() {         .          .     18:   var c chan int         .          .     19:   for {         .          .     20:           select {    11.61s     56.83s     21:           case v := &lt;-c:         .          .     22:                   fmt.Printf(&quot;recv from chan, value:%v\n&quot;, v)         .          .     23:           default:         .          .     24:         .          .     25:           }         .          .     26:   }(pprof) </code></pre><p><strong>通过<code>list funcName</code>就可以定位到哪行代码的问题了！当然<code>PProf</code>工具的用途多的是这里就是一个简单的例子，我这篇博文就写到这里，有兴趣的后面再更新，或者关注我的公众号:<code>go_code</code>谢谢里面有干货哦<del>~</del>🤩</strong></p>]]></content>
    
    
    
    <tags>
      
      <tag>Go</tag>
      
      <tag>PPorf</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Testing of Golang</title>
    <link href="/2020/05/15/Testing-of-Golang/"/>
    <url>/2020/05/15/Testing-of-Golang/</url>
    
    <content type="html"><![CDATA[<h2 id="GoLang的单元测试"><a href="#GoLang的单元测试" class="headerlink" title="GoLang的单元测试"></a>GoLang的单元测试</h2><p>在我们写代码的过程中要养成一个写注释和写单元测试的习惯，这样可以提高我们的代码健壮性和可读性，今天我们就来介绍一下<code>golang</code>的<code>test</code>这个工具。</p><h2 id="go-test命令介绍"><a href="#go-test命令介绍" class="headerlink" title="go test命令介绍"></a><code>go test</code>命令介绍</h2><p>Go语言中的测试依赖<code>go test</code>命令。编写测试代码和编写普通的Go代码过程是类似的，并不需要学习新的语法、规则或工具。</p><p><code>go test</code>命令是一个按照一定约定和组织的测试代码的驱动程序。在包目录内，所有以<code>_test.go</code>为后缀名的源代码文件都是<code>go test</code>测试的一部分，不会被<code>go build</code>编译到最终的可执行文件中。</p><p>在<code>*_test.go</code>文件中有三种类型的函数，单元测试函数、基准测试函数和示例函数。</p><table><thead><tr><th>类型</th><th>格式</th><th>作用</th></tr></thead><tbody><tr><td>测试函数</td><td>函数名前缀为<code>Test</code></td><td>测试程序的一些逻辑行为是否正确</td></tr><tr><td>基准函数</td><td>函数名前缀为<code>Benchmark</code></td><td>测试函数的性能</td></tr><tr><td>示例函数</td><td>函数名前缀为<code>Example</code></td><td>为文档提供示例文档</td></tr></tbody></table><h2 id="测试函数"><a href="#测试函数" class="headerlink" title="测试函数"></a>测试函数</h2><blockquote><p>基本测试的一些方法说明在文档里:<a href="https://studygolang.com/static/pkgdoc/pkg/testing.htm" target="_blank" rel="noopener">https://studygolang.com/static/pkgdoc/pkg/testing.htm</a></p></blockquote><p><strong>本测试的函数<code>math.go</code></strong></p><pre><code class="go">// Copyright (c) 2020 HigKer// Open Source: MIT License// Author: SDing &lt;deen.job@qq.com&gt;// Date: 2020/5/15 - 4:16 下午package lets_36_testingfunc init() {    Math := make(map[string]func(n, m int) int, 4)    Math[&quot;add&quot;] = Add    Math[&quot;sub&quot;] = Sub    Math[&quot;multi&quot;] = Multi    Math[&quot;div&quot;] = Div}func Add(n, m int) int {    return n + m}func Sub(n, m int) int {    return n - m}func Multi(n, m int) int {    return n * m}func Div(n, m int) int {    return n / m}</code></pre><p><strong>测试函数<code>math_test.go</code>遵循官方命名规范</strong></p><pre><code class="go">// Copyright (c) 2020 HigKer// Open Source: MIT License// Author: SDing &lt;deen.job@qq.com&gt;// Date: 2020/5/15 - 4:21 下午package lets_36_testingimport &quot;testing&quot;func TestMath(t *testing.T) {    // 自定义测试结构体    type MathCase struct {        n, m, result int    }    // 自定义子测试map    testGroup := map[string]MathCase{        &quot;add&quot;:   {1, 2, 3},        &quot;sub&quot;:   {3, 1, 2},        &quot;multi&quot;: {3, 2, 6},        &quot;div&quot;:   {6, 2, 3},    }    // 测试执行函数    for name, mathCase := range testGroup {        t.Run(name, func(t *testing.T) {            s := -1            switch name {            case &quot;add&quot;:                s = Add(mathCase.n, mathCase.m)            case &quot;sub&quot;:                s = Sub(mathCase.n, mathCase.m)            case &quot;multi&quot;:                s = Multi(mathCase.n, mathCase.m)            case &quot;div&quot;:                s = Div(mathCase.n, mathCase.m)            default:                t.Fatalf(&quot;No executable testing name :%s&quot;, name)            }            if mathCase.result != s {                t.Fatalf(&quot; add computer result error， want %d , got %d&quot;, mathCase.result, s)            }        })    }}</code></pre><p><strong>在自己的terminal输入<code>go test</code>即可看到测试结果</strong></p><pre><code class="go">=== RUN   TestMath=== RUN   TestMath/sub=== RUN   TestMath/multi=== RUN   TestMath/div=== RUN   TestMath/add--- PASS: TestMath (0.00s)    --- PASS: TestMath/sub (0.00s)    --- PASS: TestMath/multi (0.00s)    --- PASS: TestMath/div (0.00s)    --- PASS: TestMath/add (0.00s)PASSok      command-line-arguments  0.004s</code></pre><blockquote><p>如果是子测试可以加 <code>-run=funcName/TestCase</code>来进行子测试</p></blockquote><h2 id="测试覆盖率"><a href="#测试覆盖率" class="headerlink" title="测试覆盖率"></a>测试覆盖率</h2><p>测试覆盖率是你的代码被测试套件覆盖的百分比。通常我们使用的都是语句的覆盖率，也就是在测试中至少被运行一次的代码占总代码的比例。Go提供内置功能来检查你的代码覆盖率。我们可以使用<code>go test -cover</code>来查看测试覆盖率，例如：</p><pre><code class="go">$: go test -coverPASScoverage: 100.0% of statementsok      Lets_Go/lets_36_testing 0.004s</code></pre><p><strong>可以看到我的代码测试覆盖率是百分之百的~</strong></p><p>Go还提供了一个额外的<code>go test -cover -coverprofile</code>参数，用来将覆盖率相关的记录信息输出到一个文件里:</p><pre><code class="go">$: go test -cover -coverprofile=testInfo.out[50 50 50 50 50 50]PASScoverage: 100.0% of statementsok      Lets_Go/lets_36_testing 0.005s</code></pre><p><strong>这样我们就把测试的信息输出到一个指定的<code>testInfo.out</code>文件里面了,然后我们通过go语言内置<code>tool</code>工具就可以生产可视化的代码覆盖率检查视图了，使用命令<code>go tool cover -html=testInfo.out</code>例如:</strong><br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1getd9mki8bj313u0u0dve.jpg" srcset="/img/loading.gif" alt=""></p><h2 id="基准测试"><a href="#基准测试" class="headerlink" title="基准测试"></a>基准测试</h2><p>基准测试是给你的程序做运行性能的测试,以Benchmark为前缀，需要一个*testing.B类型的参数b，基准测试必须要执行b.N次，这样的测试才有对照性，b.N的值是系统根据实际情况去调整的，从而保证测试的稳定性。</p><pre><code class="go">func BenchmarkAdd(b *testing.B) {    for i:=0;i&lt;b.N;i++ {        Add(i, b.N)        //fmt.Println(add)    }}</code></pre><p><strong>增加<code>-bench</code>参数，所以我们通过执行<code>go test -bench=Add</code>命令执行基准测试，输出结果如下</strong></p><pre><code class="go">$: go test -bench=Addgoos: darwingoarch: amd64pkg: Lets_Go/lets_36_testingBenchmarkAdd-4          1000000000               0.281 ns/opPASSok      Lets_Go/lets_36_testing 0.320s</code></pre><p><strong>其中BenchmarkAdd-4表示对Split函数进行基准测试，数字4表示GOMAXPROCS的值，这个对于并发基准测试很重要。10000000和203ns/op表示每次调用Add函数耗时0.281ns，这个结果是10000000次调用的平均值。</strong></p><p>还可以添加<code>-benchmem</code>参数来查看被查内存使用情况:</p><pre><code class="go">$: go test -bench=Add -benchmemgoos: darwingoarch: amd64pkg: Lets_Go/lets_36_testingBenchmarkAdd-4          1000000000               0.292 ns/op           0 B/op          0 allocs/opPASSok      Lets_Go/lets_36_testing 0.335s</code></pre><p><strong><code>0 B/op</code>表示每次操作内存分配了<code>0</code>字节，<code>0 allocs/op</code>则表示每次操作进行了<code>0</code>次内存分配,因为我写的就是一个简单加法函数😁</strong></p><blockquote><p>Good luck😜~</p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>Go</tag>
      
      <tag>Golang</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>sync.Mutex&amp;RWMutex笔记</title>
    <link href="/2020/05/12/sync-Mutex-RWMutex%E7%AC%94%E8%AE%B0/"/>
    <url>/2020/05/12/sync-Mutex-RWMutex%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="Go-中锁的使用"><a href="#Go-中锁的使用" class="headerlink" title="Go 中锁的使用"></a>Go 中锁的使用</h2><ol><li>互斥锁</li><li>读写锁</li></ol><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gepy9opn7vj30ge0gq3yz.jpg" srcset="/img/loading.gif" alt=""></p><h2 id="互斥锁"><a href="#互斥锁" class="headerlink" title="互斥锁"></a>互斥锁</h2><p>并发程序对公共资源访问的限制最常见的就是使用互斥锁的方式。在Go中，sync.Mutex 提供了互斥锁的实现。</p><p>简单使用示例：</p><pre><code class="go">func main() {var mutex sync.Mutexcount := 0for r := 0; r &lt; 50; r++ {   go func() {           mutex.Lock()           count += 1           mutex.Unlock()       }()   }   time.Sleep(time.Second)   fmt.Println(&quot;the count is : &quot;, count)}</code></pre><p>当执行了 <code>mutex.Lock()</code> 操作后，如果有另外一个 <code>goroutine</code> 又执行了上锁操作，那么该操作被被阻塞，直到该互斥锁恢复到解锁状态。</p><h2 id="读写锁"><a href="#读写锁" class="headerlink" title="读写锁"></a>读写锁</h2><p>顾名思义，读写锁是对读写操作进行加锁。需要注意的是多个读操作之间不存在互斥关系，这样提高了对共享资源的访问效率。</p><p>Go中读写锁由 <code>sync.RWMutex</code>提供，主要包括：</p><ul><li><p><code>func (rw *RWMutex) Lock()</code></p></li><li><p><code>func (rw *RWMutex) RLock()</code></p></li><li><p><code>func (rw *RWMutex) RLocker() Locker</code></p></li><li><p><code>func (rw *RWMutex) RUnlock()</code></p></li><li><p><code>func (rw *RWMutex) Unlock()</code></p></li></ul><p>其中 <code>Lock()</code> 即“写锁定”，调用了“写锁定”后，不能有其他<code>goroutine</code>进行读或者写操作。 <code>Unlock()</code> 即“写解锁”，调用了“写解锁”后会唤醒所有因为要进行“读锁定（即:<code>RLock()</code>）” 而被阻塞的 <code>goroutine</code>。</p><p><code>RLock()</code>为“读锁定”，调用“读锁定”后，不能有其他<code>goroutine</code>进行写操作，但是可以进行读操作。<code>RUnlock()</code> 为“读解锁”，调用“读解锁”后，会唤醒一个因为要进行“写锁定”而被阻塞的<code>goroutine</code>。</p><p>简单使用示例：</p><pre><code class="go">package mainimport (&quot;fmt&quot;&quot;sync&quot;&quot;time&quot;)func main() {    var mutex sync.RWMutex    arr := []int{1, 2, 3}    go func() {       fmt.Println(&quot;Try to lock writing operation.&quot;)       mutex.Lock()       fmt.Println(&quot;Writing operation is locked.&quot;)       arr = append(arr, 4)       fmt.Println(&quot;Try to unlock writing operation.&quot;)       mutex.Unlock()       fmt.Println(&quot;Writing operation is unlocked.&quot;)   }()   go func() {       fmt.Println(&quot;Try to lock reading operation.&quot;)       mutex.RLock()       fmt.Println(&quot;The reading operation is locked.&quot;)       fmt.Println(&quot;The len of arr is : &quot;, len(arr))       fmt.Println(&quot;Try to unlock reading operation.&quot;)       mutex.RUnlock()       fmt.Println(&quot;The reading operation is unlocked.&quot;)   }()   time.Sleep(time.Second * 2)   return}</code></pre><p>运行以上示例，观察输出结果，你将能够比较直观的感受到读写锁的作用。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Go</tag>
      
      <tag>读写锁</tag>
      
      <tag>Mutex</tag>
      
      <tag>RWMutex</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TCP黏包</title>
    <link href="/2020/05/11/TCP%E9%BB%8F%E5%8C%85/"/>
    <url>/2020/05/11/TCP%E9%BB%8F%E5%8C%85/</url>
    
    <content type="html"><![CDATA[<h2 id="什么是TCP粘包"><a href="#什么是TCP粘包" class="headerlink" title="什么是TCP粘包?"></a>什么是TCP粘包?</h2><ol><li>通常我在自己写程序的时候写一个基于<code>tcp</code>协议通讯的程序时就会发生这种问题。</li><li>我们在<code>Client</code>端快速发送数据包并且这个包数据是随机,时间间隔也很短,然后<code>tcp</code>的<code>Server</code>拿到数据就会是我们之前<code>Client</code>发送的多条数据包合在的一起的，导致我们不能区分具体是哪个数据包的。</li><li>为什么会产生这种原因就是因为<code>tcp</code>在传输的是<code>字节流协议</code>。<blockquote><p>上面是我个人自学习过程中总结的大家可以看看这个链接上的解释:<br><a href="https://www.liwenzhou.com/posts/Go/15_socket/" target="_blank" rel="noopener">https://www.liwenzhou.com/posts/Go/15_socket/</a></p></blockquote></li></ol><h2 id="为什么会出现粘包"><a href="#为什么会出现粘包" class="headerlink" title="为什么会出现粘包"></a>为什么会出现粘包</h2><p>主要原因就是tcp数据传递模式是流模式，在保持长连接的时候可以进行多次的收和发。</p><p>“粘包”可发生在发送端也可发生在接收端：</p><ul><li>由Nagle算法造成的发送端的粘包：Nagle算法是一种改善网络传输效率的算法。简单来说就是当我们提交一段数据给TCP发送时，TCP并不立刻发送此段数据，而是等待一小段时间看看在等待期间是否还有要发送的数据，若有则会一次把这两段数据发送出去。</li><li>接收端接收不及时造成的接收端粘包：TCP会把接收到的数据存在自己的缓冲区中，然后通知应用层取数据。当应用层由于某些原因不能及时的把TCP的数据取出来，就会造成TCP缓冲区中存放了几段数据。</li></ul><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1geosakdma0j30on0brabp.jpg" srcset="/img/loading.gif" alt=""></p><h2 id="通过Go语言实现"><a href="#通过Go语言实现" class="headerlink" title="通过Go语言实现"></a>通过Go语言实现</h2><blockquote><p>server.go</p></blockquote><pre><code class="go">// Copyright (c) 2020 HigKer// Open Source: MIT License// Author: SDing &lt;deen.job@qq.com&gt;// Date: 2020/5/11 - 7:14 下午package mainimport (    &quot;fmt&quot;    &quot;net&quot;)func main() {    listen, err := net.Listen(&quot;tcp&quot;, &quot;localhost:9598&quot;)    if err != nil {        fmt.Println(&quot;create tcp server fail.&quot;,err)        return    }    var msg [1024]byte    for{        accept, err := listen.Accept()        if err != nil {            fmt.Println(&quot;tcp accept fail&quot;,err)            return        }        n , _ := accept.Read(msg[:])        fmt.Println(string(msg[:n]))    }}</code></pre><blockquote><p>client.go</p></blockquote><pre><code class="go">// Copyright (c) 2020 HigKer// Open Source: MIT License// Author: SDing &lt;deen.job@qq.com&gt;// Date: 2020/5/11 - 7:58 下午package mainimport (    &quot;fmt&quot;    &quot;net&quot;    &quot;time&quot;)func main() {    dial, err := net.Dial(&quot;tcp&quot;, &quot;localhost:9598&quot;)    if err != nil {        fmt.Println(&quot;connection tcp server fail.&quot;,err)    }    for i:=0;i&lt;10;i++ {        dial.Write([]byte(&quot;Hello&quot; + time.Now().Format(&quot;2006-01-02 15:04:05.0000&quot;)))    }}</code></pre><blockquote><p>OutPut</p></blockquote><pre><code> ~/Documents/GO_CODE_DEV/src/Lets_Go/lets_33_tcp_黏包  master ✚  ↵ 1  go run server.goHello2020-05-11 20:16:13.5448Hello2020-05-11 20:16:13.5449Hello2020-05-11 20:16:13.5449Hello2020-05-11 20:16:13.5449Hello2020-05-11 20:16:13.5449Hello2020-05-11 20:16:13.5449Hello2020-05-11 20:16:13.5449Hello2020-05-11 20:16:13.5449Hello2020-05-11 20:16:13.5449Hello2020-05-11 20:16:13.5449</code></pre><p><strong>可以看到所有数据包都被黏在一起了！！！！这就是我们所说的tcp黏包。</strong></p><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><blockquote><p>出现黏包的原因是因为服务器端或者消息接受者不知道我们每次发送的数据包大小！！所以我们可以自定义一个数据包格式来解决。就像我们出去约别人吃饭一样的，我们俩先商量好去哪里地方几点…..(例子有点抽象不好理解啊哈哈哈)</p></blockquote><blockquote><p>protocol.go</p></blockquote><pre><code class="go">package mainimport (    &quot;bufio&quot;    &quot;bytes&quot;    &quot;encoding/binary&quot;)// Encode 将消息编码func Encode(message string) ([]byte, error) {    // 读取消息的长度，转换成int32类型（占4个字节）    var length = int32(len(message))    // 创建缓冲区    var pkg = new(bytes.Buffer)    // 写入消息头 “小端方式写入存储”这里我也不太清楚看别人写的例子,用就完事了    err := binary.Write(pkg, binary.LittleEndian, length)    if err != nil {        return nil, err    }    // 在包头后面 追加 写入消息实体    err = binary.Write(pkg, binary.LittleEndian, []byte(message))    if err != nil {        return nil, err    }    return pkg.Bytes(), nil}// Decode 解码消息func Decode(reader *bufio.Reader) (string, error) {    // 读取消息的长度    lengthByte, _ := reader.Peek(4) // 读取前4个字节的数据    lengthBuff := bytes.NewBuffer(lengthByte)    var length int32    err := binary.Read(lengthBuff, binary.LittleEndian, &amp;length)    if err != nil {        return &quot;&quot;, err    }    // Buffered返回缓冲中现有的可读取的字节数。    if int32(reader.Buffered()) &lt; length+4 {        return &quot;&quot;, err    }    // 读取真正的消息数据    pack := make([]byte, int(4+length))    _, err = reader.Read(pack)    if err != nil {        return &quot;&quot;, err    }    return string(pack[4:]), nil}</code></pre><p><strong>接下来在服务端和客户端分别使用上面定义的Decode和Encode函数处理数据。下面有我的源代码仓库,Good Luck~😜</strong><br><strong><a href="https://github.com/higker/Lets_Go/tree/master/lets_33_tcp_%E9%BB%8F%E5%8C%85" target="_blank" rel="noopener">https://github.com/higker/Lets_Go/tree/master/lets_33_tcp_%E9%BB%8F%E5%8C%85</a></strong></p><h2 id="附加知识点"><a href="#附加知识点" class="headerlink" title="附加知识点"></a>附加知识点</h2><blockquote><p>为什么udp不会粘包?<br><a href="https://zhuanlan.zhihu.com/p/41709589?utm_source=wechat_session" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/41709589?utm_source=wechat_session</a></p></blockquote><p>1.TCP协议是面向流的协议，UDP是面向消息的协议</p><p>UDP段都是一条消息，应用程序必须以消息为单位提取数据，不能一次提取任意字节的数据</p><p>UDP具有保护消息边界，在每个UDP包中就有了消息头（消息来源地址，端口等信息），这样对于接收端来说就容易进行区分处理了。传输协议把数据当作一条独立的消息在网上传输，接收端只能接收独立的消息。接收端一次只能接收发送端发出的一个数据包,如果一次接受数据的大小小于发送端一次发送的数据大小，就会丢失一部分数据，即使丢失，接受端也不会分两次去接收</p>]]></content>
    
    
    
    <tags>
      
      <tag>Go</tag>
      
      <tag>tcp</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>谈谈并发和并行</title>
    <link href="/2020/05/08/%E8%B0%88%E8%B0%88%E5%B9%B6%E5%8F%91%E5%92%8C%E5%B9%B6%E8%A1%8C/"/>
    <url>/2020/05/08/%E8%B0%88%E8%B0%88%E5%B9%B6%E5%8F%91%E5%92%8C%E5%B9%B6%E8%A1%8C/</url>
    
    <content type="html"><![CDATA[<h2 id="什么是并发？"><a href="#什么是并发？" class="headerlink" title="什么是并发？"></a>什么是并发？</h2><p> 就是在一段时间里来回切换做多个事情，就像一个快递员在一分钟内送完我的快递又继续去送的快递，干活的就只要一个快递员(这个例子有点不好还是看下面图片吧)。<code>并发(concurrency)</code>：指在同一时刻只能有一条指令执行，但多个进程指令被快速的轮换执行，使得在宏观上具有多个进程同时执行的效果，但在微观上并不是同时执行的，只是把时间分成若干段，使多个进程快速交替的执行。<br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gelclh44mgj308704j74e.jpg" srcset="/img/loading.gif" alt=""></p><h2 id="什么并行？"><a href="#什么并行？" class="headerlink" title="什么并行？"></a>什么并行？</h2><p>在同一时刻内处理多个事情，就像一个2个快递员在同时时间内分别给你我送快递。(他们2个在同一个时间内同时干活，效率高)。<code>并行(parallel)：</code>指在同一时刻，有多条指令在多个处理器上同时执行。所以无论从微观还是从宏观来看，二者都是一起执行的。<br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1geldwbtva1j308p04mgln.jpg" srcset="/img/loading.gif" alt=""></p><h2 id="什么区别？"><a href="#什么区别？" class="headerlink" title="什么区别？"></a>什么区别？</h2><p>并行在多处理器系统中存在，而并发可以在单处理器和多处理器系统中都存在，并发能够在单处理器系统中存在是因为并发是并行的假象，并行要求程序能够同时执行多个操作，而并发只是要求程序假装同时执行多个操作（每个小时间片执行一个操作，多个操作快速切换执行）。</p><p>当有多个线程在操作时，如果系统只有一个 CPU，则它根本不可能真正同时进行一个以上的线程，它只能把 CPU 运行时间划分成若干个时间段，再将时间段分配给各个线程执行，在一个时间段的线程代码运行时,其它线程处于挂起状态.这种方式我们称之为并发（Concurrent）。</p><p>当系统有一个以上 CPU 时，则线程的操作有可能非并发。当一个 CPU 执行一个线程时，另一个 CPU 可以执行另一个线程，两个线程互不抢占 CPU 资源，可以同时进行，这种方式我们称之为并行（Parallel）。<br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1geld285ybjj30i40ea750.jpg" srcset="/img/loading.gif" alt=""></p><p><strong>看图非常容易理解：</strong><br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1geld3btporj30o909faaz.jpg" srcset="/img/loading.gif" alt=""><br>上图中将一个任务中的三个步骤取柴、运柴、卸柴划分成了独立的小任务，有取柴的老鼠，有运柴的老鼠，有卸柴烧火的老鼠。<br>如果上图中所有的老鼠都是同一只，那么是串行并发的，如果是不同的多只老鼠，那么是并行并发的。</p><h4 id="并行和串行："><a href="#并行和串行：" class="headerlink" title="并行和串行："></a>并行和串行：</h4><ul><li>串行：一次只能取得一个任务并执行这一个任务</li><li>并行：可以同时通过多进程/多线程的方式取得多个任务，并以多进程或多线程的方式同时执行这些任务</li><li>注意点：<ul><li>如果是单进程/单线程的并行，那么效率比串行更差</li><li>如果只有单核cpu，多进程并行并没有提高效率</li><li>从任务队列上看，由于同时从队列中取得多个任务并执行，相当于将一个长任务队列变成了短队列</li></ul></li></ul><h3 id="并发："><a href="#并发：" class="headerlink" title="并发："></a>并发：</h3><ul><li>并发是一种现象：同时运行多个程序或多个任务需要被处理的现象</li><li>这些任务可能是并行执行的，也可能是串行执行的，和CPU核心数无关，是操作系统进程调度和CPU上下文切换达到的结果</li><li>解决大并发的一个思路是将大任务分解成多个小任务：</li><li>可能要使用一些数据结构来避免切分成多个小任务带来的问题</li><li>可以多进程/多线程并行的方式去执行这些小任务达到高效率</li><li>或者以单进程/单线程配合多路复用执行这些小任务来达到高效率</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Go</tag>
      
      <tag>计算机知识</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Go之高阶函数</title>
    <link href="/2020/04/24/Go%E4%B9%8B%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0/"/>
    <url>/2020/04/24/Go%E4%B9%8B%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="什么是高阶函数-？"><a href="#什么是高阶函数-？" class="headerlink" title="什么是高阶函数??？"></a>什么是高阶函数??？</h1><p>一个函数可以接收另一个函数作为参数，这种函数就称之为高阶函数，编写高阶函数，就是让函数的参，能够接收别的函数，函数式编程就是指这种高度抽象的编程范式。</p><h1 id="高阶函数的特点"><a href="#高阶函数的特点" class="headerlink" title="高阶函数的特点"></a>高阶函数的特点</h1><ul><li>变量可以指向函数</li></ul><p><code>函数本身也可以赋值给变量，即：变量可以指向函数</code></p><pre><code class="go">package main//go语言导包import (    m &quot;Lets_Go/lets_18_package/math&quot; //起别名  自定义包名也就是给“math“起个别名    &quot;fmt&quot;)func main() {    multip := m.SimpleCompute(&quot;+&quot;)    fmt.Println(multip(2, 8))}</code></pre><ul><li>返回值也是函数</li></ul><pre><code class="go">/*    闭包高阶函数*/func SimpleCompute(Symbol string) func(n, m int) int {    switch Symbol {    case &quot;+&quot;:        return func(n, m int) int {            return n + m        }    case &quot;-&quot;:        return func(n, m int) int {            return n - m        }    case &quot;*&quot;:        return func(n, m int) int {            return n * m        }    case &quot;/&quot;:        return func(n, m int) int {            return n / m        }    default:        panic(&quot;compute symbol invalid.&quot;)    }}</code></pre><ul><li>函数可作为参数</li></ul><p><code>既然变量可以指向函数，函数的参数能接收变量，那么一个函数就可以接收另一个函数作为参数一个最简单的高阶函数</code></p><pre><code class="go">package mainimport &quot;fmt&quot;//go语言中的闭包操作func main() {    f1(f3(f2, 5, 5))}//1.限制函数类型的函数func f1(f func()) {    f()}//2.现在的需求就要把我们的f2函数传递到f1函数里面进行调用func f2(a, b int) int {    fmt.Println(&quot;f2() am f1() exec succeed&quot;)    fmt.Println(&quot;a + b = ? 在f3中的匿名函数中调用:&quot;)    return a + b}//3.通过闭包解决func f3(fn func(int, int) int, a, b int) (Rf func()) { //返回值名字必须是“Rf” 不想用就不需要写直接写返回值类型    //这个就是取一个中间变量存储 并且 在函数里面声明一个匿名无返回值的函数    // := 这里如果函数返回值已经在函数名上写着了 就不能使用 := 因为已经在创建函数时已经声明了    Rf = func() {        sum := fn(a, b) //接受返回值 然后打印        fmt.Println(&quot;sum=&quot;, sum)    }    return Rf}</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Go</tag>
      
      <tag>golang</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Go之channel浅入了解</title>
    <link href="/2020/04/23/Go%E4%B9%8Bchannel%E6%B5%85%E5%85%A5%E4%BA%86%E8%A7%A3/"/>
    <url>/2020/04/23/Go%E4%B9%8Bchannel%E6%B5%85%E5%85%A5%E4%BA%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="什么是Go语言的channel？"><a href="#什么是Go语言的channel？" class="headerlink" title="什么是Go语言的channel？"></a>什么是Go语言的channel？</h2><blockquote><p>在Go语言中,Channel即指通道类型。有时也用它来直接指代可以传递某种类型的值的通道。</p></blockquote><p>channel是Go语言中的一个核心类型，可以把它看成管道。并发核心单元通过它就可以发送或者接收数据进行通讯，这在一定程度上又进一步降低了编程的难度。<br>channel是一个数据类型，主要用来解决协程的同步问题以及协程之间数据共享（数据传递）的问题。<br>goroutine运行在相同的地址空间，因此访问共享内存必须做好同步。goroutine 奉行通过通信来共享内存，而不是共享内存来通信。<br>引⽤类型 channel可用于多个 goroutine 通讯。其内部实现了同步，确保并发安全。</p><p>虽然可以使用共享内存进行数据交换，但是共享内存在不同的goroutine中容易发生竞态问题。为了保证数据交换的正确性，必须使用互斥量对内存进行加锁，这种做法势必造成性能问题。</p><p>Go语言的并发模型是CSP（Communicating Sequential Processes），提倡通过通信共享内存而不是通过共享内存而实现通信。</p><p>如果说goroutine是Go程序并发的执行体，channel就是它们之间的连接。channel是可以让一个goroutine发送特定值到另一个goroutine的通信机制。</p><p>Go 语言中的通道（channel）是一种特殊的类型。通道像一个传送带或者队列，总是遵循先入先出（First In First Out）的规则，保证收发数据的顺序。每一个通道都是一个具体类型的导管，也就是声明channel的时候需要为其指定元素类型。<br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge3kqrcxboj30dw072mxe.jpg" srcset="/img/loading.gif" alt=""></p><h2 id="属性和基本操作"><a href="#属性和基本操作" class="headerlink" title="属性和基本操作"></a>属性和基本操作</h2><ul><li>基于通道的通讯是在多个Goroutine之间进行同步的重要手段。而针对通道的操作本身也是同步的。</li><li>在同一时刻，仅有一个Goroutine能向一个通道发送元素值</li><li>同时也仅有一个Goroutine能从它那里接收元素值。</li><li>通道相当于一个FIFO先进先出的消息队列。</li><li>通道中的元素值都具有原子性。它们是不可被分割的。通道中的每一个元素都只可能被某一个Goroutine接收。已被接收的元素值会立刻被从通道中删除。</li></ul><h2 id="有缓冲的channel"><a href="#有缓冲的channel" class="headerlink" title="有缓冲的channel"></a>有缓冲的channel</h2><p>有缓冲的通道（buffered channel）是一种在被接收前能存储一个或者多个数据值的通道。<br>这种类型的通道并不强制要求 goroutine 之间必须同时完成发送和接收。通道会阻塞发送和接收动作的条件也不同。<br>只有通道中没有要接收的值时，接收动作才会阻塞。<br>只有通道没有可用缓冲区容纳被发送的值时，发送动作才会阻塞。<br>这导致有缓冲的通道和无缓冲的通道之间的一个很大的不同：无缓冲的通道保证进行发送和接收的 goroutine 会在同一时间进行数据交换；有缓冲的通道没有这种保证。</p><p>示例图如下：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge3kigisnoj30m70ewtaf.jpg" srcset="/img/loading.gif" alt=""></p><p>在第 1 步，右侧的 goroutine 正在从通道接收一个值。<br>在第 2 步，右侧的这个 goroutine独立完成了接收值的动作，而左侧的 goroutine 正在发送一个新值到通道里。<br>在第 3 步，左侧的goroutine 还在向通道发送新值，而右侧的 goroutine 正在从通道接收另外一个值。这个步骤里的两个操作既不是同步的，也不会互相阻塞。<br>最后，在第 4 步，所有的发送和接收都完成，而通道里还有几个值，也有一些空间可以存更多的值。</p><p><em>使用示例</em></p><pre><code class="go">// Copyright (c) 2020 HigKer// Open Source: MIT License// Author: SDing &lt;deen.job@qq.com&gt;// Date: 2020/4/22 - 3:15 下午package mainimport (    &quot;fmt&quot;    &quot;sync&quot;)var wg sync.WaitGroup//go语言的channel 通道//go语言使用goroutine执行多个task任务//操作一个map或者其他变量或者内存时会发生数据竞争//使用go里面使用channel来解决并发并行来解决func main() {    //channel类型 相当于一个队列 first in -&gt; first out 先进先出原则    //定义一个channel类型 channel是一个引用类型 需要开辟空间    var ch chan int    fmt.Println(ch == nil) //true    //初始化一个无缓冲区的通道    ch = make(chan int)    wg.Add(1)    go func (){        defer wg.Done()        x := &lt;- ch        fmt.Println(x)    }()    ch &lt;- 2048 //会卡死main的主goroutine从而不能让程序进行执行    fmt.Println(ch) //无缓冲区的channel不能放入值    fmt.Println(ch == nil) //false    //初始化一个带缓冲区的通道    bufChan := make(chan int,8)    fmt.Println(cap(bufChan)) //8 CAP可以换取通道缓冲区大小    //发送值 使用这个&quot;&lt;-符号&quot; 获取也一样    bufChan &lt;- 1024    num := &lt;-bufChan    fmt.Println(bufChan)    fmt.Println(num)    wg.Wait()}</code></pre><blockquote><p>上面部分内容转载于链接：<a href="https://www.jianshu.com/p/e611d30feb9d" target="_blank" rel="noopener">https://www.jianshu.com/p/e611d30feb9d</a></p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>Golang</tag>
      
      <tag>goroutine</tag>
      
      <tag>channel</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Go之goroutine浅入了解</title>
    <link href="/2020/04/22/Go%E4%B9%8Bgoroutine%E6%B5%85%E5%85%A5%E4%BA%86%E8%A7%A3/"/>
    <url>/2020/04/22/Go%E4%B9%8Bgoroutine%E6%B5%85%E5%85%A5%E4%BA%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="1-什么goroutine"><a href="#1-什么goroutine" class="headerlink" title="1.什么goroutine?"></a>1.什么goroutine?</h1><p>goroutine是Go并行设计的核心。goroutine说到底其实就是协程，它比线程更小，十几个goroutine可能体现在底层就是五六个线程，Go语言内部帮你实现了这些goroutine之间的内存共享。执行goroutine只需极少的栈内存(大概是4~5KB)，当然会根据相应的数据伸缩。也正因为如此，可同时运行成千上万个并发任务。goroutine比thread更易用、更高效、更轻便。<br>一般情况下，一个普通计算机跑几十个线程就有点负载过大了，但是同样的机器却可以轻松地让成百上千个goroutine进行资源竞争。</p><p>Goroutine是建立在线程之上的轻量级的抽象。它允许我们以非常低的代价在同一个地址空间中并行地执行多个函数或者方法。相比于线程，它的创建和销毁的代价要小很多，并且它的调度是独立于线程的。在golang中创建一个goroutine非常简单，使用“go”关键字即可：</p><pre><code class="go">// Copyright (c) 2020 HigKer// Open Source: MIT License// Author: SDing &lt;deen.job@qq.com&gt;// Date: 2020/4/21 - 6:09 下午package mainimport (    &quot;fmt&quot;    &quot;math/rand&quot;    &quot;sync&quot;    &quot;time&quot;)var (    //这个是go里面用来配置goroutine使用的    //只能操作指针 因为他是一个struct 值类型！！！    wg sync.WaitGroup    //存放我们for的数字方便观察    sls []int)//go语言中的多个goroutine//WaitGroupfunc main() {    fmt.Println(&quot;goroutine begin:&quot;, sls)    for i := 0; i &lt;= 10; i++ {        // fatal error: all goroutines are asleep - deadlock!        //注意这里加的1不是i如果是i的话每次都是加的不一样然后出现👆的异常！！！        wg.Add(1)        go task(i)    }    fmt.Println(&quot;InService:&quot;, sls)    wg.Wait()    fmt.Println(&quot;End Over:&quot;, sls)}// fatal error: all goroutines are asleep - deadlock!func task(num int) {    defer func() {        wg.Done()    }()    //随机休眠几毫秒    time.Sleep(duration())    //将for循环的i存入到切片中我们等下好观察,    //打印控制台太慢了回漏掉    sls = append(sls, num) //不安全}//生成一个随机的等待时间 = 毫秒func duration() time.Duration {    rand.Seed(time.Now().UnixNano())    return time.Millisecond * time.Duration(rand.Intn(3000))}</code></pre><h3 id="output"><a href="#output" class="headerlink" title="output"></a>output</h3><pre><code class="shell">goroutine begin: []InService: []End Over: [4 10 9 8 6 1 0 3 2]</code></pre><blockquote><p>会出现一个问题多次运行你会发现有时候结果的slice会少一个数,这是因为，和线程一样，golang的主函数（其实也跑在一个goroutine中）并不会等待其它goroutine结束。如果主goroutine结束了，所有其它goroutine都将结束,所有后面我们就会将到go的channel来解决问题~数据竞争和数据共享,多个goroutine之间的通讯问题。</p></blockquote><h3 id="Goroutine与线程的区别"><a href="#Goroutine与线程的区别" class="headerlink" title="Goroutine与线程的区别"></a>Goroutine与线程的区别</h3><p>许多人认为goroutine比线程运行得更快，这是一个误解。Goroutine并不会更快，它只是增加了更多的并发性。当一个goroutine被阻塞（比如等待IO），golang的scheduler会调度其它可以执行的goroutine运行。与线程相比，它有以下几个优点：</p><h3 id="内存消耗更少："><a href="#内存消耗更少：" class="headerlink" title="内存消耗更少："></a>内存消耗更少：</h3><p>Goroutine所需要的内存通常只有2kb，而线程则需要1Mb（500倍）。<br>内存消耗更少：</p><h3 id="创建与销毁的开销更小"><a href="#创建与销毁的开销更小" class="headerlink" title="创建与销毁的开销更小"></a>创建与销毁的开销更小</h3><p>由于线程创建时需要向操作系统申请资源，并且在销毁时将资源归还，因此它的创建和销毁的开销比较大。相比之下，goroutine的创建和销毁是由go语言在运行时自己管理的，因此开销更低。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge2mjdldsfj30hs0a5q3h.jpg" srcset="/img/loading.gif" alt=""></p><h3 id="切换开销更小"><a href="#切换开销更小" class="headerlink" title="切换开销更小"></a>切换开销更小</h3><p>这是goroutine于线程的主要区别，也是golang能够实现高并发的主要原因。线程的调度方式是抢占式的，如果一个线程的执行时间超过了分配给它的时间片，就会被其它可执行的线程抢占。在线程切换的过程中需要保存/恢复所有的寄存器信息，比如16个通用寄存器，PC（Program Counter），SP（Stack Pointer），段寄存器等等。</p><p>而goroutine的调度是协同式的，它不会直接地与操作系统内核打交道。当goroutine进行切换的时候，之后很少量的寄存器需要保存和恢复（PC和SP）。因此gouroutine的切换效率更高。</p><h3 id="Goroutine的调度"><a href="#Goroutine的调度" class="headerlink" title="Goroutine的调度"></a>Goroutine的调度</h3><p>真如前面提到的，goroutine的调度方式是协同式的。在协同式调度中，没有时间片的概念。为了并行执行goroutine，调度器会在以下几个时间点对其进行切换：</p><h3 id="Channel接受或者发送会造成阻塞的消息"><a href="#Channel接受或者发送会造成阻塞的消息" class="headerlink" title="Channel接受或者发送会造成阻塞的消息"></a>Channel接受或者发送会造成阻塞的消息</h3><p>当一个新的goroutine被创建时<br>可以造成阻塞的系统调用，如文件和网络操作<br>垃圾回收<br>下面让我们来看一下调度器具体是如何工作的。Golang调度器中有三个概念</p><ul><li>Processor（P）</li><li>OSThread（M）</li><li>Goroutines（G）</li></ul><p>在一个Go程序中，可用的线程数是通过GOMAXPROCS来设置的，默认值是可用的CPU核数。我们可以用runtime包动态改变这个值。OSThread调度在processor上，goroutines调度在OSThreads上<br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge2ml5ze1wj30hs08at98.jpg" srcset="/img/loading.gif" alt=""><br>Golang的调度器可以利用多processor资源，在任意时刻，M个goroutine需要被调度到N个OS threads上，同时这些threads运行在至多GOMAXPROCS个processor上（N &lt;= GOMAXPROCS）。Go scheduler将可运行的goroutines分配到多个运行在一个或多个processor上的OS threads上。</p><p>每个processor有一个本地goroutine队列。同时有一个全局的goroutine队列。每个OSThread都会被分配给一个processor。最多只能有GOMAXPROCS个processor，每个processor同时只能执行一个OSThread。Scheculer可以根据需要创建OSThread。<br><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge2mlz0si1j30hs08j74l.jpg" srcset="/img/loading.gif" alt=""><br>在每一轮调度中，scheduler找到一个可以运行的goroutine并执行直到其被阻塞,由此可见，操作系统的一个线程下可以并发执行上千个goroutine，每个goroutine所占用的资源和切换开销都很小，因此，goroutine是golang适合高并发场景的重要原因。</p>]]></content>
    
    
    
    <tags>
      
      <tag>go</tag>
      
      <tag>golang</tag>
      
      <tag>goroutine</tag>
      
      <tag>channel</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mac上配置Golang开发环境</title>
    <link href="/2020/03/13/Mac%E4%B8%8A%E9%85%8D%E7%BD%AEGolang%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/"/>
    <url>/2020/03/13/Mac%E4%B8%8A%E9%85%8D%E7%BD%AEGolang%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/</url>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2020/03/13/JYRMzrxeKlLNp26.jpg" srcset="/img/loading.gif" alt="Golang"></p><blockquote><p><strong>📝本篇文章记录我本人在Mac上配置Go开发环境的笔记📒</strong></p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul><li><a href="https://github.com/golang/go" target="_blank" rel="noopener">Go Source Code</a></li><li>Go是Google开发的一种静态强类型、编译型、并发型，并具有垃圾回收功能的编程语言。 罗伯特·格瑞史莫，罗勃·派克及肯·汤普逊于2007年9月开始设计Go，稍后Ian Lance Taylor、Russ Cox加入项目。Go是基于Inferno操作系统所开发的。</li><li>Go的语法接近C语言，但对于变量的声明有所不同。Go支持垃圾回收功能。Go的并行计算模型是以东尼·霍尔的通信顺序进程（CSP）为基础，采取类似模型的其他语言包括Occam和Limbo，[2]，但它也具有pipeline模型的特征，比如通道传输。在1.8版本中开放插件（Plugin）的支持，这意味着现在能从Go中动态加载部分函数。与C++相比，Go并不包括如枚举、异常处理、继承、泛型、断言、虚函数等功能，但增加了 切片(Slice) 型、并发、管道、垃圾回收功能、接口等特性的语言级支持[2]。Go 2.0版本将支持泛型[7]，对于断言的存在，则持负面态度，同时也为自己不提供类型继承来辩护。不同于Java，Go原生提供了关联数组（也称为哈希表（Hashes）或字典（Dictionaries）），就像字符串类型一样。</li></ul><h2 id="下载-amp-安装"><a href="#下载-amp-安装" class="headerlink" title="下载 &amp; 安装"></a>下载 &amp; 安装</h2><ul><li><a href="https://go.dev/" target="_blank" rel="noopener">Go Dev</a></li><li>浏览器🔛打开上面👆链接下载官方安装包<br><img src="https://i.loli.net/2020/03/13/XosNZdnTVtr3auA.jpg" srcset="/img/loading.gif" alt="Xnip2020-03-13_15-54-37.jpg"></li><li>下载的时候根据自己系统版本下载对应的安装包</li><li>下载完成之后双击安装包即可安装</li></ul><h2 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h2><p>我本人Mac是使用的<code>iTerm2 + Oh My Zsh + zsh</code>所以我只需要编辑我的<code>~/.zshrc</code>文件📃即可.</p><ul><li>通过vscode打开<pre><code class="bash">code ~/.zshrc</code></pre></li><li>在文件最后添加修改配置<pre><code class="bash">##go语言环境配置 ##go 语言安装主根目录export GOROOT=/usr/local/go#GOPATH是自己的go项目路径，自定义设置export GOPATH=/Users/ding/Documents/GO_CODE_DEV# 启用 Go Modules 功能export GO111MODULE=on# 配置 GOPROXY 环境变量 &amp; 国内网络环境大家都知道不多说export GOPROXY=https://goproxy.io,directexport PATH=$PATH:$GOBIN:$GOPATH/bin:GO111MODULE:GOPROXY</code></pre></li><li>然后使用<code>source ~/.zshrc</code>文件</li><li>在terminal里面输入<code>go env</code>即可查看环境<br><img src="https://i.loli.net/2020/03/13/3hrCzjTS5D4MlKJ.jpg" srcset="/img/loading.gif" alt="Xnip2020-03-13_16-07-43.jpg"></li></ul><h2 id="VSCode配置"><a href="#VSCode配置" class="headerlink" title="VSCode配置"></a>VSCode配置</h2><p>我个人使用vscode比较多一点,Goland在开发大型项目才使用<br><strong>VS Code官方下载地址：<a href="https://code.visualstudio.com/Download" target="_blank" rel="noopener">https://code.visualstudio.com/Download</a></strong><br>三大主流平台都支持，请根据自己的电脑平台选择对应的安装包。双击下载好的安装文件，双击安装即可。</p><ul><li><h4 id="安装Go语言开发工具包"><a href="#安装Go语言开发工具包" class="headerlink" title="安装Go语言开发工具包"></a>安装Go语言开发工具包</h4><p>在座Go语言开发的时候为我们提供诸如代码提示、代码自动补全等功能。<br>Windows平台按下Ctrl+Shift+P，Mac平台按Command+Shift+P，这个时候VS Code界面会弹出一个输入框，如下图：<br><img src="https://www.liwenzhou.com/images/Go/00_config_VSCode/15535662106193.jpg" srcset="/img/loading.gif" alt="GO"><br>我们在这个输入框中输入<code>&gt;go:install</code>，下面会自动搜索相关命令，我们选择<code>Go:Install/Update Tools</code>这个命令<br><img src="https://www.liwenzhou.com/images/Go/00_config_VSCode/15535659707162.jpg" srcset="/img/loading.gif" alt="GOINSTALL"><br>选中并会回车执行该命令（或者使用鼠标点击该命令）<br><img src="https://www.liwenzhou.com/images/Go/00_config_VSCode/15535665573387.jpg" srcset="/img/loading.gif" alt="golang"></p></li><li><h4 id="配置代码片段快捷键"><a href="#配置代码片段快捷键" class="headerlink" title="配置代码片段快捷键"></a>配置代码片段快捷键</h4><p>还是按Ctrl/Command+Shift+P,按下图输入<code>&gt;snippets</code>，选择命令并执行<br><img src="https://www.liwenzhou.com/images/Go/00_config_VSCode/15535687503862.jpg" srcset="/img/loading.gif" alt="GO"><br>配置<code>snippets</code>代码片段模本<br><img src="https://www.liwenzhou.com/images/Go/00_config_VSCode/15535688890224.jpg" srcset="/img/loading.gif" alt="GO SNIPPETS"><br>大家可以简单看下上面的注释，介绍了主要用法：</p><pre><code class="json">    “这里放个名字”:{      &quot;prefix&quot;: &quot;这个是快捷键&quot;,      &quot;body&quot;: &quot;这里是按快捷键插入的代码片段&quot;,      &quot;description&quot;: &quot;这里放提示信息的描述&quot;    }</code></pre><p> 其中<code>$0</code>表示最终光标提留的位置。举个例子，我这里创建了两个快捷方式，一个是输入<code>pln</code>就会在编辑器中插入<code>fmt.Println()</code>代码；输入<code>plf</code>，就会插入<code>fmt.Printf(&quot;&quot;)</code>代码。</p><pre><code class="json">  {    &quot;println&quot;:{      &quot;prefix&quot;: &quot;pln&quot;,      &quot;body&quot;:&quot;fmt.Println($0)&quot;,      &quot;description&quot;: &quot;println&quot;    },    &quot;printf&quot;:{      &quot;prefix&quot;: &quot;plf&quot;,      &quot;body&quot;: &quot;fmt.Printf(\&quot;$0\&quot;)&quot;,      &quot;description&quot;: &quot;printf&quot;    }  }</code></pre><p>添加如上配置后，保存。 我们打开一个go文件，测试一下效果：<br> <img src="https://www.liwenzhou.com/images/Go/00_config_VSCode/demo1.gif" srcset="/img/loading.gif" alt="fmt.println"></p></li></ul><p><strong>好了配置完成,开始愉快的coding吧👨‍💻‍Good luck~😜</strong>   </p>]]></content>
    
    
    
    <tags>
      
      <tag>Go</tag>
      
      <tag>Golang</tag>
      
      <tag>Mac</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hexo集成gitalk评论组件</title>
    <link href="/2020/03/08/Hexo%E9%9B%86%E6%88%90gitalk%E8%AF%84%E8%AE%BA%E7%BB%84%E4%BB%B6/"/>
    <url>/2020/03/08/Hexo%E9%9B%86%E6%88%90gitalk%E8%AF%84%E8%AE%BA%E7%BB%84%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/BayMRX/Blog_source@master/img/hexo.jpg" srcset="/img/loading.gif" alt="hexo"></p><blockquote><p><strong>一个基于 Github Issue 和 Preact 开发的评论插件,下面我们就来使用gitalk来实现hexo的评论功能。</strong></p></blockquote><h4 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h4><ul><li><a href="https://gitalk.github.io/" target="_blank" rel="noopener">Gitalk开源地址</a></li></ul><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><ul><li>使用 GitHub 登录</li><li>支持多语言 [en, zh-CN, zh-TW, es-ES, fr, ru]</li><li>支持个人或组织</li><li>无干扰模式（设置 distractionFreeMode 为 true 开启）</li><li>快捷键提交评论 （cmd|ctrl + enter）</li></ul><p><a href="https://github.com/gitalk/gitalk/blob/master/readme.md" target="_blank" rel="noopener">Readme</a><br><a href="https://gitalk.github.io" target="_blank" rel="noopener">在线示例</a></p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>两种方式</p><ul><li>直接引入</li></ul><pre><code class="html">  &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css&quot;&gt;  &lt;script src=&quot;https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js&quot;&gt;&lt;/script&gt;  &lt;!-- or --&gt;  &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/gitalk/dist/gitalk.css&quot;&gt;  &lt;script src=&quot;https://unpkg.com/gitalk/dist/gitalk.min.js&quot;&gt;&lt;/script&gt;</code></pre><ul><li>npm 安装</li></ul><pre><code class="sh">npm i --save gitalk</code></pre><pre><code class="js">import &#39;gitalk/dist/gitalk.css&#39;import Gitalk from &#39;gitalk&#39;</code></pre><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>添加一个容器：</p><pre><code class="html">&lt;div id=&quot;gitalk-container&quot;&gt;&lt;/div&gt;</code></pre><p>用下面的 Javascript 代码来生成 gitalk 插件：</p><pre><code class="js">var gitalk = new Gitalk({  clientID: &#39;GitHub Application Client ID&#39;,  clientSecret: &#39;GitHub Application Client Secret&#39;,  repo: &#39;GitHub repo&#39;,  owner: &#39;GitHub repo owner&#39;,  admin: [&#39;GitHub repo owner and collaborators, only these guys can initialize github issues&#39;],  id: location.pathname,      // Ensure uniqueness and length less than 50  distractionFreeMode: false  // Facebook-like distraction free mode})gitalk.render(&#39;gitalk-container&#39;)</code></pre><p>需要 <strong>GitHub Application</strong>，如果没有 <a href="https://github.com/settings/applications/new" target="_blank" rel="noopener">点击这里申请</a>，<code>Authorization callback URL</code> 填写当前使用插件页面的域名。</p><p><strong>⚠️更多帮助在官方文档,可以在官方链接里查询。</strong></p><h2 id="设置GitHub"><a href="#设置GitHub" class="headerlink" title="设置GitHub"></a>设置GitHub</h2><ul><li>在个人设置里面创建一个Application<br><img src="https://i.loli.net/2020/03/08/fKqE9JHALBXMnN7.jpg" srcset="/img/loading.gif" alt="Xnip2020-03-07_12-46-47.jpg"></li><li>然后我们需要的ClientID和Token<br><img src="https://i.loli.net/2020/03/08/vAx7wkRFhsNyqtX.jpg" srcset="/img/loading.gif" alt="clientId"></li><li>创建一个存放lssue仓库<br><img src="https://i.loli.net/2020/03/08/jeBmlY2RD6Fi35E.jpg" srcset="/img/loading.gif" alt="Xnip2020-03-07_13-00-14.jpg"><h2 id="在Hexo里使用"><a href="#在Hexo里使用" class="headerlink" title="在Hexo里使用"></a>在Hexo里使用</h2><blockquote><p>这里我使用的主题里面已经集成了gitalk,所有我们直接修改配置文件就可以了。根据个人配置方法不同查询各种文档！！！</p></blockquote></li></ul><p>下面是我的配置文件📃</p><pre><code class="yaml"># Gitalk# You can get yout ClientID and ClientSecret from https://github.com/settings/applications/new# More info available at https://github.com/gitalk/gitalk#optionsgitalk:  clientID: 这里填写你之前application的clientID # GitHub Application Client ID  clientSecret: 之前申请的 # GitHub Application Client Secret  repo: ibyte.me # The repo to store comments  owner: deencode # GitHub repository owner. Can be personal user or organization.  admin: deencode # GitHub repo owner and collaborators, only these guys can initialize github issues, should be like &quot;[&#39;admin&#39;]&quot; or &quot;[&#39;admin1&#39;,&#39;admin2&#39;]&quot;  id: location.pathname # The unique id of the page. Length must less than 50.  language: zh-CN # Localization language key, en, zh-CN and zh-TW are currently available.  labels: &quot;[&#39;Gitalk&#39;]&quot; # GitHub issue labels.  perPage: 15 # Pagination size, with maximum 100.  pagerDirection: last # Comment sorting direction, available values are last and first.  distractionFreeMode: false # Facebook-like distraction free mode.  createIssueManually: true # By default, Gitalk will create a corresponding github issue for your every single page automatically when the logined user is belong to the admin users. You can create it manually by setting this option to true.</code></pre><p><strong>配置方法不同有所差异!!根据自己使用的插件查询官方文档进行的配置！！！Good Luck😜</strong></p>]]></content>
    
    
    
    <tags>
      
      <tag>Hexo</tag>
      
      <tag>gitalk</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用Hexo搭建Blog</title>
    <link href="/2020/03/06/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BABlog/"/>
    <url>/2020/03/06/%E4%BD%BF%E7%94%A8Hexo%E6%90%AD%E5%BB%BABlog/</url>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2020/03/06/cOUwFstAaTeyGQk.jpg" srcset="/img/loading.gif" alt="Hexo&amp;GithubPage.jpg"></p><h3 id="1-相关网站"><a href="#1-相关网站" class="headerlink" title="1. 相关网站"></a>1. 相关网站</h3><ul><li>[Github]: <a href="https://github.com/" target="_blank" rel="noopener">https://github.com/</a></li><li>[Hexo]: <a href="https://hexo.io/" target="_blank" rel="noopener">https://hexo.io/</a></li><li>[Hexo Theme]: <a href="https://hexo.io/themes/" target="_blank" rel="noopener">https://hexo.io/themes/</a></li><li>[Hexo Docs]: <a href="https://hexo.io/docs/" target="_blank" rel="noopener">https://hexo.io/docs/</a></li></ul><h3 id="2-创建Github-Key"><a href="#2-创建Github-Key" class="headerlink" title="2. 创建Github Key"></a>2. 创建Github Key</h3><p>设置你的用户名称与邮件地址，如果是第一次使用git的话</p><pre><code class="bash">$ git config --global user.name &quot;John Doe&quot;$ git config --global user.email johndoe@example.com</code></pre><p>使用ssh-keygen生成私钥和公钥<br>命令如下：</p><pre><code class="bash">$ ssh-keygen -t rsa</code></pre><p>然后到你GitHub 设置里面添加你刚才生成的xx.pub的内容到Key里面.</p><pre><code class="bash">$ ssh -T git@github.com</code></pre><p>登录Github，点击头像下的settings，添加ssh,新建一个new ssh key，将id_rsa.pub文件里的内容复制上去。</p><p>输入ssh -T <a href="mailto:git@github.com">git@github.com</a>，测试添加ssh是否成功。如果看到Hi后面是你的用户名，就说明成功了。</p><p>这里你可以去创建一个public的仓库来存储等下Hexo相关的文件.</p><h3 id="3-安装NPM和Hexo"><a href="#3-安装NPM和Hexo" class="headerlink" title="3. 安装NPM和Hexo"></a>3. 安装NPM和Hexo</h3><ul><li>[NodeJS网站]: <a href="http://nodejs.cn/" target="_blank" rel="noopener">http://nodejs.cn/</a></li></ul><p>通过上面的网站下载对应的系统版本的NodeJS安装包.</p><ol><li><strong>设置NPM源到淘宝源</strong></li></ol><pre><code class="bash">   npm config set registry https://registry.npmjs.org/</code></pre><ol start="2"><li><p><strong>NPM安装HexoCli</strong></p><p>输入npm install hexo -g，开始安装Hexo</p></li></ol><pre><code class="bash">   npm install hexo -g</code></pre><ol start="3"><li><p><strong>初始化Hexo</strong></p><p>输入hexo -v，检查hexo是否安装成功</p><p>输入hexo init，初始化该文件夹（有点漫长的等待…）看到后面的“Start blogging with Hexo！”就说明初始化好了</p><p>输入npm install，安装所需要的组件</p></li></ol><pre><code class="bash">$ hexo -v$ hexo init$ hexo install        </code></pre><h3 id="4-配置config-yml文件"><a href="#4-配置config-yml文件" class="headerlink" title="4.配置config.yml文件"></a>4.配置config.yml文件</h3><p><strong>PS:这里根据自己需要自己配置，你可以去查询官方文档！！</strong></p><pre><code class="bash">deploy:  type: git  repo: git@github.com:Deencode/ibyte.me.git  branch: master</code></pre><h3 id="5-生成静态文件-amp-上传到GitHub仓库"><a href="#5-生成静态文件-amp-上传到GitHub仓库" class="headerlink" title="5.生成静态文件&amp;上传到GitHub仓库"></a>5.生成静态文件&amp;上传到GitHub仓库</h3><p><strong>🎉: 到了这一步就差不多了，运行下面命令就可以生成文件了.</strong></p><pre><code class="bash">#生成文件$ hexo generate#监视文件变动Hexo 能够监视文件变动并立即重新生成静态文件#在生成时会比对文件的 SHA1 checksum，只有变动的文件才会写入。$ hexo generate --watch# 本地运行$ hexo server#您可执行下列的其中一个命令，让 Hexo 在生成完毕后自动部署网站，两个命令的作用是相同的。$ hexo deploy</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Hexo</tag>
      
      <tag>github</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
